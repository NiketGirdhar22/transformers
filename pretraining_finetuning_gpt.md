# Pre-Training GPT

As GPT is auto-regressive, hence the pre-training tasks in BERT are not suitable for it:

### 1. **Masked Language Model (MLM) in BERT vs. Autoregressive in GPT**:
   - **BERT (Masked Language Model)**: In BERT, a portion of the input tokens is randomly masked, and the model is trained to predict those masked tokens. Since BERT processes text bidirectionally (i.e., it looks at the context on both sides of the masked word), it learns the meaning of a token by considering both its left and right context.
   - **GPT (Autoregressive)**: GPT, on the other hand, generates tokens one by one in a left-to-right manner. It predicts the next token in the sequence, conditioning only on the tokens to the left. This is a key difference: GPT doesn't need to mask tokens, as its training objective is simply to predict the next token given the previous context.

### 2. **Next Sentence Prediction (NSP) in BERT vs. GPT's Training**:
   - **BERT (Next Sentence Prediction)**: BERT was trained with a task where it was given pairs of sentences and had to predict if the second sentence followed the first in the original text. This task was designed to help BERT understand relationships between sentences and is particularly useful for tasks like question answering and natural language inference.
   - **GPT**: GPT does not have the need for NSP since it is autoregressive and focuses on predicting the next token in a sequence. There's no inherent need to understand the relationship between two separate sentences because the model generates text sequentially without any notion of sentence boundaries or adjacent sentence tasks.

## Pretraining for GPT

GPT is pretrained on auto-regressive language model task with a dataset - WebText(large text corpora - 40GB text).

This data is scrapped was scrapped from Reddit and hence it has a lot of textual data which tends to introduce bias in GPT-2. This bias can be noticed in the text generated by GPT-2 during its dempnstartion.

This bias needs to be handled well before the architecture is used any further or this bias can be tranfereed through transfer learning.

---

## Few-shot Learning

| **Zero-shot learning** | **One-shot learning** | **Few-shot learning** |
|------------------------|-----------------------|-----------------------|
| The model is tasked with performing a task it has never seen before, using no prior examples or training on that specific task. It generalizes knowledge from other tasks or domains. | The model is required to learn a task after being given only a single example. It must generalize from that one example to correctly perform the task. | The model is given a few (typically 2â€“10) examples to learn from, using them to generalize and make predictions for new, unseen data. |
| Example: A model recognizes objects it hasn't been explicitly trained on by understanding their descriptions in natural language. | Example: A facial recognition system identifies a person after seeing only one image of them. | Example: A model learns to classify new objects after seeing a few labeled examples of those objects. |
| Often used in natural language processing and computer vision tasks like image captioning and text classification. | Common in computer vision (e.g., object recognition) and classification tasks. | Typically used in situations where labeled data is limited, such as medical imaging or rare events classification. |
| Example methods: GPT (language models), CLIP (vision-language models). | Example methods: Matching Networks, Siamese Networks. | Example methods: Prototypical Networks, Meta-Learning algorithms. |

Few-shot learning means that we provide the model with a task description and as many examples as we can that would be able to fit in the context window of the model [maxnimum number of tokens the model can input].

For GPT-2 the context window is 1024 tokens.

Zero and One shot learning is subset of Few-shot leanring where we give zero or one example to the model.

---

## Codes

- [Pretraining GPT in play using Python](codes/gpt/pretrain_gpt.ipynb)
- [Fine-Tuning GPT for style completion using Python](codes/gpt/gpt_style.ipynb)
- [Fine-Tuning GPT for code generation using Python](codes/gpt/gpt_code_dictation.ipynb)