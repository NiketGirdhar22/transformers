# Transformers

Flow:

- [Introduction to Attention and Language Models](introduction.md)
- [Transformer Artchitecture and attention usage for processing text](how_transformers_use_attention.md)
- [Transfer Learning](transfer_learning.md)
- **BERT**
    - [Understanding BERT](bert.md)
    - [Understanding Pretraining and Fine-Tuning BERT](bert_pretrain_finetune.md)
    - [Understanding the derivative architectures of BERT](derivatives_of_BERT.md)
    - **Fine-Tuning BERT**
        - [BERT for Sequence Classification](bert_for_sequence_classification.md)
        - [BERT for Token Classification](bert_for_token_classification.md)
        - [BERT for Question Answering](bert_for_question_answer.md)
    - [Siamese BERT networks for semantic searching](Siamese_BERT_networks_for_semantic_searching.md)
- **GPT**
    - [Understaing GPT](gpt.md)
    - [Pre-Training in GPT](pretraining_gpt.md)
    - [Teaching GPT multiple tasks at once with prompt engineering](codes/gpt/teaching_gpt_multiple_tasks_with_prompt_engg.ipynb)
- **T5**
    - [Understanding T5](t5.md)
    - [T5 implemetation and testing in python](codes/t5/base_t5.ipynb)
    - [T5 implementation for Abstractive Summarization in python](codes/t5/t5_for_abstractive_summarization.ipynb)
    