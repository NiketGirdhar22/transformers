### **Cross Attention**:
- **Definition**: Cross attention is a variation of the attention mechanism where elements from one sequence are attended to by elements from a different sequence.
- **Purpose**: It allows the model to focus on one sequence (e.g., source sentence) while processing elements from another sequence (e.g., target sentence).
  
For example:
- In **machine translation**, the model might use cross attention to relate words in a source language sequence (e.g., "cat") to words in a target language sequence (e.g., "gato" in Spanish).
  
### **Transformer: Self-Attention vs. Cross-Attention**
Transformers use both **self-attention** and **cross-attention**, depending on the task:

1. **Self-Attention**:
   - Used in the **encoder** of the Transformer model.
   - Each token attends to every other token in the same sequence (e.g., input sentence), capturing context and dependencies within that sequence.

2. **Cross-Attention**:
   - Used in the **decoder** of the Transformer model.
   - The decoder uses cross attention to attend to the encoderâ€™s output (source sequence) while generating the target sequence. Here, each token in the decoder attends to the encoder's representations, aligning the source and target sequences.

### **Summary**:
- **Self-attention** is used within the encoder and decoder to capture internal context.
- **Cross-attention** is used in the decoder to relate the source sequence (from the encoder) to the target sequence (being generated by the decoder).

So, **Transformers** are based on both **self-attention** (within sequences) and **cross-attention** (across sequences).

1[transformer architecture](images/transformer_arch.png)