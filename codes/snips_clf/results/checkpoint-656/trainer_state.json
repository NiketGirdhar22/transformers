{
  "best_metric": 1.584693193435669,
  "best_model_checkpoint": "./snips_clf/results/checkpoint-656",
  "epoch": 2.0,
  "eval_steps": 50,
  "global_step": 656,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.003048780487804878,
      "grad_norm": 0.9622209668159485,
      "learning_rate": 2.3889154323936934e-08,
      "loss": 1.9193,
      "step": 1
    },
    {
      "epoch": 0.006097560975609756,
      "grad_norm": 1.1136653423309326,
      "learning_rate": 4.777830864787387e-08,
      "loss": 1.9456,
      "step": 2
    },
    {
      "epoch": 0.009146341463414634,
      "grad_norm": 2.012254238128662,
      "learning_rate": 7.16674629718108e-08,
      "loss": 1.9882,
      "step": 3
    },
    {
      "epoch": 0.012195121951219513,
      "grad_norm": 1.35887610912323,
      "learning_rate": 9.555661729574773e-08,
      "loss": 1.9208,
      "step": 4
    },
    {
      "epoch": 0.01524390243902439,
      "grad_norm": 1.7767150402069092,
      "learning_rate": 1.1944577161968468e-07,
      "loss": 1.9301,
      "step": 5
    },
    {
      "epoch": 0.018292682926829267,
      "grad_norm": 1.479033350944519,
      "learning_rate": 1.433349259436216e-07,
      "loss": 1.9623,
      "step": 6
    },
    {
      "epoch": 0.021341463414634148,
      "grad_norm": 1.357845425605774,
      "learning_rate": 1.6722408026755853e-07,
      "loss": 1.9643,
      "step": 7
    },
    {
      "epoch": 0.024390243902439025,
      "grad_norm": 0.8007515072822571,
      "learning_rate": 1.9111323459149547e-07,
      "loss": 1.9415,
      "step": 8
    },
    {
      "epoch": 0.027439024390243903,
      "grad_norm": 1.198912501335144,
      "learning_rate": 2.150023889154324e-07,
      "loss": 1.9535,
      "step": 9
    },
    {
      "epoch": 0.03048780487804878,
      "grad_norm": 1.7378391027450562,
      "learning_rate": 2.3889154323936937e-07,
      "loss": 1.9889,
      "step": 10
    },
    {
      "epoch": 0.03353658536585366,
      "grad_norm": 1.528782606124878,
      "learning_rate": 2.6278069756330625e-07,
      "loss": 1.9472,
      "step": 11
    },
    {
      "epoch": 0.036585365853658534,
      "grad_norm": 1.3359522819519043,
      "learning_rate": 2.866698518872432e-07,
      "loss": 1.9363,
      "step": 12
    },
    {
      "epoch": 0.039634146341463415,
      "grad_norm": 1.0131381750106812,
      "learning_rate": 3.1055900621118013e-07,
      "loss": 1.9448,
      "step": 13
    },
    {
      "epoch": 0.042682926829268296,
      "grad_norm": 1.2646815776824951,
      "learning_rate": 3.3444816053511706e-07,
      "loss": 1.959,
      "step": 14
    },
    {
      "epoch": 0.04573170731707317,
      "grad_norm": 1.3211686611175537,
      "learning_rate": 3.58337314859054e-07,
      "loss": 1.9403,
      "step": 15
    },
    {
      "epoch": 0.04878048780487805,
      "grad_norm": 1.681921124458313,
      "learning_rate": 3.8222646918299094e-07,
      "loss": 1.9031,
      "step": 16
    },
    {
      "epoch": 0.051829268292682924,
      "grad_norm": 1.5285249948501587,
      "learning_rate": 4.0611562350692793e-07,
      "loss": 1.944,
      "step": 17
    },
    {
      "epoch": 0.054878048780487805,
      "grad_norm": 1.5222811698913574,
      "learning_rate": 4.300047778308648e-07,
      "loss": 1.927,
      "step": 18
    },
    {
      "epoch": 0.057926829268292686,
      "grad_norm": 1.0062505006790161,
      "learning_rate": 4.5389393215480175e-07,
      "loss": 1.9614,
      "step": 19
    },
    {
      "epoch": 0.06097560975609756,
      "grad_norm": 1.22685968875885,
      "learning_rate": 4.777830864787387e-07,
      "loss": 1.9503,
      "step": 20
    },
    {
      "epoch": 0.06402439024390244,
      "grad_norm": 1.430120587348938,
      "learning_rate": 5.016722408026756e-07,
      "loss": 1.9664,
      "step": 21
    },
    {
      "epoch": 0.06707317073170732,
      "grad_norm": 1.5091720819473267,
      "learning_rate": 5.255613951266125e-07,
      "loss": 1.9671,
      "step": 22
    },
    {
      "epoch": 0.0701219512195122,
      "grad_norm": 1.91354238986969,
      "learning_rate": 5.494505494505495e-07,
      "loss": 1.9529,
      "step": 23
    },
    {
      "epoch": 0.07317073170731707,
      "grad_norm": 1.3267364501953125,
      "learning_rate": 5.733397037744864e-07,
      "loss": 1.9423,
      "step": 24
    },
    {
      "epoch": 0.07621951219512195,
      "grad_norm": 0.9140681028366089,
      "learning_rate": 5.972288580984234e-07,
      "loss": 1.9326,
      "step": 25
    },
    {
      "epoch": 0.07926829268292683,
      "grad_norm": 1.398987054824829,
      "learning_rate": 6.211180124223603e-07,
      "loss": 1.9437,
      "step": 26
    },
    {
      "epoch": 0.08231707317073171,
      "grad_norm": 1.697360634803772,
      "learning_rate": 6.450071667462972e-07,
      "loss": 1.9477,
      "step": 27
    },
    {
      "epoch": 0.08536585365853659,
      "grad_norm": 1.4342522621154785,
      "learning_rate": 6.688963210702341e-07,
      "loss": 1.9386,
      "step": 28
    },
    {
      "epoch": 0.08841463414634146,
      "grad_norm": 1.227698802947998,
      "learning_rate": 6.92785475394171e-07,
      "loss": 1.9559,
      "step": 29
    },
    {
      "epoch": 0.09146341463414634,
      "grad_norm": 1.42353093624115,
      "learning_rate": 7.16674629718108e-07,
      "loss": 1.9966,
      "step": 30
    },
    {
      "epoch": 0.09451219512195122,
      "grad_norm": 1.276287317276001,
      "learning_rate": 7.405637840420449e-07,
      "loss": 1.9534,
      "step": 31
    },
    {
      "epoch": 0.0975609756097561,
      "grad_norm": 1.4006229639053345,
      "learning_rate": 7.644529383659819e-07,
      "loss": 1.9527,
      "step": 32
    },
    {
      "epoch": 0.10060975609756098,
      "grad_norm": 1.0602002143859863,
      "learning_rate": 7.883420926899189e-07,
      "loss": 1.9661,
      "step": 33
    },
    {
      "epoch": 0.10365853658536585,
      "grad_norm": 0.9262228012084961,
      "learning_rate": 8.122312470138559e-07,
      "loss": 1.9547,
      "step": 34
    },
    {
      "epoch": 0.10670731707317073,
      "grad_norm": 1.1916615962982178,
      "learning_rate": 8.361204013377926e-07,
      "loss": 1.9795,
      "step": 35
    },
    {
      "epoch": 0.10975609756097561,
      "grad_norm": 1.334334373474121,
      "learning_rate": 8.600095556617296e-07,
      "loss": 1.9419,
      "step": 36
    },
    {
      "epoch": 0.11280487804878049,
      "grad_norm": 1.5545203685760498,
      "learning_rate": 8.838987099856666e-07,
      "loss": 1.9478,
      "step": 37
    },
    {
      "epoch": 0.11585365853658537,
      "grad_norm": 1.148835301399231,
      "learning_rate": 9.077878643096035e-07,
      "loss": 1.925,
      "step": 38
    },
    {
      "epoch": 0.11890243902439024,
      "grad_norm": 0.9770706295967102,
      "learning_rate": 9.316770186335405e-07,
      "loss": 1.9417,
      "step": 39
    },
    {
      "epoch": 0.12195121951219512,
      "grad_norm": 1.3688756227493286,
      "learning_rate": 9.555661729574775e-07,
      "loss": 1.9661,
      "step": 40
    },
    {
      "epoch": 0.125,
      "grad_norm": 1.2181131839752197,
      "learning_rate": 9.794553272814141e-07,
      "loss": 1.9664,
      "step": 41
    },
    {
      "epoch": 0.12804878048780488,
      "grad_norm": 1.3802183866500854,
      "learning_rate": 1.0033444816053512e-06,
      "loss": 1.9459,
      "step": 42
    },
    {
      "epoch": 0.13109756097560976,
      "grad_norm": 1.5476137399673462,
      "learning_rate": 1.0272336359292883e-06,
      "loss": 1.9689,
      "step": 43
    },
    {
      "epoch": 0.13414634146341464,
      "grad_norm": 1.2079963684082031,
      "learning_rate": 1.051122790253225e-06,
      "loss": 1.9519,
      "step": 44
    },
    {
      "epoch": 0.13719512195121952,
      "grad_norm": 1.3908147811889648,
      "learning_rate": 1.0750119445771621e-06,
      "loss": 1.9487,
      "step": 45
    },
    {
      "epoch": 0.1402439024390244,
      "grad_norm": 1.446913242340088,
      "learning_rate": 1.098901098901099e-06,
      "loss": 1.9371,
      "step": 46
    },
    {
      "epoch": 0.14329268292682926,
      "grad_norm": 1.2889872789382935,
      "learning_rate": 1.1227902532250359e-06,
      "loss": 1.9686,
      "step": 47
    },
    {
      "epoch": 0.14634146341463414,
      "grad_norm": 0.8432591557502747,
      "learning_rate": 1.1466794075489728e-06,
      "loss": 1.9457,
      "step": 48
    },
    {
      "epoch": 0.14939024390243902,
      "grad_norm": 1.3165030479431152,
      "learning_rate": 1.1705685618729096e-06,
      "loss": 1.9464,
      "step": 49
    },
    {
      "epoch": 0.1524390243902439,
      "grad_norm": 1.5917041301727295,
      "learning_rate": 1.1944577161968467e-06,
      "loss": 1.9593,
      "step": 50
    },
    {
      "epoch": 0.15548780487804878,
      "grad_norm": 1.242082953453064,
      "learning_rate": 1.2183468705207836e-06,
      "loss": 1.9362,
      "step": 51
    },
    {
      "epoch": 0.15853658536585366,
      "grad_norm": 1.5561304092407227,
      "learning_rate": 1.2422360248447205e-06,
      "loss": 1.9341,
      "step": 52
    },
    {
      "epoch": 0.16158536585365854,
      "grad_norm": 1.013962984085083,
      "learning_rate": 1.2661251791686574e-06,
      "loss": 1.9337,
      "step": 53
    },
    {
      "epoch": 0.16463414634146342,
      "grad_norm": 1.2071741819381714,
      "learning_rate": 1.2900143334925945e-06,
      "loss": 1.9285,
      "step": 54
    },
    {
      "epoch": 0.1676829268292683,
      "grad_norm": 1.2972325086593628,
      "learning_rate": 1.3139034878165314e-06,
      "loss": 1.9406,
      "step": 55
    },
    {
      "epoch": 0.17073170731707318,
      "grad_norm": 1.2667003870010376,
      "learning_rate": 1.3377926421404683e-06,
      "loss": 1.9311,
      "step": 56
    },
    {
      "epoch": 0.17378048780487804,
      "grad_norm": 0.9401330351829529,
      "learning_rate": 1.3616817964644054e-06,
      "loss": 1.9355,
      "step": 57
    },
    {
      "epoch": 0.17682926829268292,
      "grad_norm": 1.1516022682189941,
      "learning_rate": 1.385570950788342e-06,
      "loss": 1.9464,
      "step": 58
    },
    {
      "epoch": 0.1798780487804878,
      "grad_norm": 1.2778100967407227,
      "learning_rate": 1.4094601051122791e-06,
      "loss": 1.9242,
      "step": 59
    },
    {
      "epoch": 0.18292682926829268,
      "grad_norm": 1.7733532190322876,
      "learning_rate": 1.433349259436216e-06,
      "loss": 1.9801,
      "step": 60
    },
    {
      "epoch": 0.18597560975609756,
      "grad_norm": 1.1387778520584106,
      "learning_rate": 1.4572384137601529e-06,
      "loss": 1.9177,
      "step": 61
    },
    {
      "epoch": 0.18902439024390244,
      "grad_norm": 1.4622206687927246,
      "learning_rate": 1.4811275680840898e-06,
      "loss": 1.9185,
      "step": 62
    },
    {
      "epoch": 0.19207317073170732,
      "grad_norm": 1.2856990098953247,
      "learning_rate": 1.5050167224080269e-06,
      "loss": 1.9556,
      "step": 63
    },
    {
      "epoch": 0.1951219512195122,
      "grad_norm": 1.1144261360168457,
      "learning_rate": 1.5289058767319638e-06,
      "loss": 1.9334,
      "step": 64
    },
    {
      "epoch": 0.19817073170731708,
      "grad_norm": 1.4428510665893555,
      "learning_rate": 1.5527950310559006e-06,
      "loss": 1.9613,
      "step": 65
    },
    {
      "epoch": 0.20121951219512196,
      "grad_norm": 1.2152093648910522,
      "learning_rate": 1.5766841853798377e-06,
      "loss": 1.9568,
      "step": 66
    },
    {
      "epoch": 0.20426829268292682,
      "grad_norm": 1.603003740310669,
      "learning_rate": 1.6005733397037744e-06,
      "loss": 1.9616,
      "step": 67
    },
    {
      "epoch": 0.2073170731707317,
      "grad_norm": 1.1111464500427246,
      "learning_rate": 1.6244624940277117e-06,
      "loss": 1.949,
      "step": 68
    },
    {
      "epoch": 0.21036585365853658,
      "grad_norm": 1.1285006999969482,
      "learning_rate": 1.6483516483516484e-06,
      "loss": 1.9393,
      "step": 69
    },
    {
      "epoch": 0.21341463414634146,
      "grad_norm": 1.4412013292312622,
      "learning_rate": 1.6722408026755853e-06,
      "loss": 1.9371,
      "step": 70
    },
    {
      "epoch": 0.21646341463414634,
      "grad_norm": 1.5901235342025757,
      "learning_rate": 1.6961299569995224e-06,
      "loss": 1.9191,
      "step": 71
    },
    {
      "epoch": 0.21951219512195122,
      "grad_norm": 1.249669075012207,
      "learning_rate": 1.7200191113234592e-06,
      "loss": 1.9754,
      "step": 72
    },
    {
      "epoch": 0.2225609756097561,
      "grad_norm": 1.0984989404678345,
      "learning_rate": 1.7439082656473961e-06,
      "loss": 1.9097,
      "step": 73
    },
    {
      "epoch": 0.22560975609756098,
      "grad_norm": 1.7193337678909302,
      "learning_rate": 1.7677974199713332e-06,
      "loss": 1.9686,
      "step": 74
    },
    {
      "epoch": 0.22865853658536586,
      "grad_norm": 1.5866273641586304,
      "learning_rate": 1.7916865742952701e-06,
      "loss": 1.927,
      "step": 75
    },
    {
      "epoch": 0.23170731707317074,
      "grad_norm": 1.2322672605514526,
      "learning_rate": 1.815575728619207e-06,
      "loss": 1.919,
      "step": 76
    },
    {
      "epoch": 0.2347560975609756,
      "grad_norm": 1.3303370475769043,
      "learning_rate": 1.839464882943144e-06,
      "loss": 1.9225,
      "step": 77
    },
    {
      "epoch": 0.23780487804878048,
      "grad_norm": 0.9922779202461243,
      "learning_rate": 1.863354037267081e-06,
      "loss": 1.9293,
      "step": 78
    },
    {
      "epoch": 0.24085365853658536,
      "grad_norm": 1.5917788743972778,
      "learning_rate": 1.8872431915910176e-06,
      "loss": 1.9816,
      "step": 79
    },
    {
      "epoch": 0.24390243902439024,
      "grad_norm": 1.5521645545959473,
      "learning_rate": 1.911132345914955e-06,
      "loss": 1.9809,
      "step": 80
    },
    {
      "epoch": 0.24695121951219512,
      "grad_norm": 1.3208634853363037,
      "learning_rate": 1.935021500238892e-06,
      "loss": 1.9643,
      "step": 81
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.9845821857452393,
      "learning_rate": 1.9589106545628283e-06,
      "loss": 1.9421,
      "step": 82
    },
    {
      "epoch": 0.2530487804878049,
      "grad_norm": 1.805677890777588,
      "learning_rate": 1.9827998088867656e-06,
      "loss": 1.9637,
      "step": 83
    },
    {
      "epoch": 0.25609756097560976,
      "grad_norm": 1.329845905303955,
      "learning_rate": 2.0066889632107025e-06,
      "loss": 1.9253,
      "step": 84
    },
    {
      "epoch": 0.25914634146341464,
      "grad_norm": 0.9773482084274292,
      "learning_rate": 2.0305781175346394e-06,
      "loss": 1.9286,
      "step": 85
    },
    {
      "epoch": 0.2621951219512195,
      "grad_norm": 1.344436764717102,
      "learning_rate": 2.0544672718585767e-06,
      "loss": 1.9631,
      "step": 86
    },
    {
      "epoch": 0.2652439024390244,
      "grad_norm": 1.4722480773925781,
      "learning_rate": 2.078356426182513e-06,
      "loss": 1.9576,
      "step": 87
    },
    {
      "epoch": 0.2682926829268293,
      "grad_norm": 1.5130747556686401,
      "learning_rate": 2.10224558050645e-06,
      "loss": 1.9702,
      "step": 88
    },
    {
      "epoch": 0.27134146341463417,
      "grad_norm": 1.4299893379211426,
      "learning_rate": 2.1261347348303873e-06,
      "loss": 1.9693,
      "step": 89
    },
    {
      "epoch": 0.27439024390243905,
      "grad_norm": 1.2989434003829956,
      "learning_rate": 2.1500238891543242e-06,
      "loss": 1.9715,
      "step": 90
    },
    {
      "epoch": 0.2774390243902439,
      "grad_norm": 1.9055125713348389,
      "learning_rate": 2.173913043478261e-06,
      "loss": 1.9095,
      "step": 91
    },
    {
      "epoch": 0.2804878048780488,
      "grad_norm": 1.6924209594726562,
      "learning_rate": 2.197802197802198e-06,
      "loss": 1.9478,
      "step": 92
    },
    {
      "epoch": 0.28353658536585363,
      "grad_norm": 1.173169493675232,
      "learning_rate": 2.221691352126135e-06,
      "loss": 1.9282,
      "step": 93
    },
    {
      "epoch": 0.2865853658536585,
      "grad_norm": 1.710693120956421,
      "learning_rate": 2.2455805064500718e-06,
      "loss": 1.945,
      "step": 94
    },
    {
      "epoch": 0.2896341463414634,
      "grad_norm": 1.3130134344100952,
      "learning_rate": 2.269469660774009e-06,
      "loss": 1.9424,
      "step": 95
    },
    {
      "epoch": 0.2926829268292683,
      "grad_norm": 1.271868348121643,
      "learning_rate": 2.2933588150979455e-06,
      "loss": 1.9504,
      "step": 96
    },
    {
      "epoch": 0.29573170731707316,
      "grad_norm": 1.3181531429290771,
      "learning_rate": 2.3172479694218824e-06,
      "loss": 1.9404,
      "step": 97
    },
    {
      "epoch": 0.29878048780487804,
      "grad_norm": 1.3056014776229858,
      "learning_rate": 2.3411371237458193e-06,
      "loss": 1.9376,
      "step": 98
    },
    {
      "epoch": 0.3018292682926829,
      "grad_norm": 1.6357600688934326,
      "learning_rate": 2.3650262780697566e-06,
      "loss": 1.9781,
      "step": 99
    },
    {
      "epoch": 0.3048780487804878,
      "grad_norm": 1.6848654747009277,
      "learning_rate": 2.3889154323936935e-06,
      "loss": 1.9404,
      "step": 100
    },
    {
      "epoch": 0.3079268292682927,
      "grad_norm": 0.9506048560142517,
      "learning_rate": 2.41280458671763e-06,
      "loss": 1.954,
      "step": 101
    },
    {
      "epoch": 0.31097560975609756,
      "grad_norm": 0.9668936133384705,
      "learning_rate": 2.4366937410415673e-06,
      "loss": 1.9363,
      "step": 102
    },
    {
      "epoch": 0.31402439024390244,
      "grad_norm": 1.4606791734695435,
      "learning_rate": 2.460582895365504e-06,
      "loss": 1.9297,
      "step": 103
    },
    {
      "epoch": 0.3170731707317073,
      "grad_norm": 1.04721200466156,
      "learning_rate": 2.484472049689441e-06,
      "loss": 1.9336,
      "step": 104
    },
    {
      "epoch": 0.3201219512195122,
      "grad_norm": 1.2171670198440552,
      "learning_rate": 2.508361204013378e-06,
      "loss": 1.9476,
      "step": 105
    },
    {
      "epoch": 0.3231707317073171,
      "grad_norm": 1.2751015424728394,
      "learning_rate": 2.5322503583373148e-06,
      "loss": 1.9096,
      "step": 106
    },
    {
      "epoch": 0.32621951219512196,
      "grad_norm": 1.2193056344985962,
      "learning_rate": 2.5561395126612517e-06,
      "loss": 1.959,
      "step": 107
    },
    {
      "epoch": 0.32926829268292684,
      "grad_norm": 1.2362452745437622,
      "learning_rate": 2.580028666985189e-06,
      "loss": 1.9437,
      "step": 108
    },
    {
      "epoch": 0.3323170731707317,
      "grad_norm": 1.4060970544815063,
      "learning_rate": 2.603917821309126e-06,
      "loss": 1.9434,
      "step": 109
    },
    {
      "epoch": 0.3353658536585366,
      "grad_norm": 0.9737282991409302,
      "learning_rate": 2.6278069756330627e-06,
      "loss": 1.9218,
      "step": 110
    },
    {
      "epoch": 0.3384146341463415,
      "grad_norm": 1.5233432054519653,
      "learning_rate": 2.6516961299569996e-06,
      "loss": 1.9773,
      "step": 111
    },
    {
      "epoch": 0.34146341463414637,
      "grad_norm": 1.2265393733978271,
      "learning_rate": 2.6755852842809365e-06,
      "loss": 1.9395,
      "step": 112
    },
    {
      "epoch": 0.3445121951219512,
      "grad_norm": 1.4105302095413208,
      "learning_rate": 2.6994744386048734e-06,
      "loss": 1.9562,
      "step": 113
    },
    {
      "epoch": 0.3475609756097561,
      "grad_norm": 1.0087634325027466,
      "learning_rate": 2.7233635929288107e-06,
      "loss": 1.9419,
      "step": 114
    },
    {
      "epoch": 0.35060975609756095,
      "grad_norm": 0.9560137987136841,
      "learning_rate": 2.747252747252747e-06,
      "loss": 1.9606,
      "step": 115
    },
    {
      "epoch": 0.35365853658536583,
      "grad_norm": 1.3747683763504028,
      "learning_rate": 2.771141901576684e-06,
      "loss": 1.9128,
      "step": 116
    },
    {
      "epoch": 0.3567073170731707,
      "grad_norm": 1.8699897527694702,
      "learning_rate": 2.7950310559006214e-06,
      "loss": 1.9825,
      "step": 117
    },
    {
      "epoch": 0.3597560975609756,
      "grad_norm": 0.8862254023551941,
      "learning_rate": 2.8189202102245582e-06,
      "loss": 1.936,
      "step": 118
    },
    {
      "epoch": 0.3628048780487805,
      "grad_norm": 1.2432398796081543,
      "learning_rate": 2.842809364548495e-06,
      "loss": 1.9601,
      "step": 119
    },
    {
      "epoch": 0.36585365853658536,
      "grad_norm": 1.3888888359069824,
      "learning_rate": 2.866698518872432e-06,
      "loss": 1.9385,
      "step": 120
    },
    {
      "epoch": 0.36890243902439024,
      "grad_norm": 1.117327094078064,
      "learning_rate": 2.890587673196369e-06,
      "loss": 1.9582,
      "step": 121
    },
    {
      "epoch": 0.3719512195121951,
      "grad_norm": 1.319507122039795,
      "learning_rate": 2.9144768275203058e-06,
      "loss": 1.9188,
      "step": 122
    },
    {
      "epoch": 0.375,
      "grad_norm": 1.1676721572875977,
      "learning_rate": 2.938365981844243e-06,
      "loss": 1.9459,
      "step": 123
    },
    {
      "epoch": 0.3780487804878049,
      "grad_norm": 1.0852755308151245,
      "learning_rate": 2.9622551361681795e-06,
      "loss": 1.9162,
      "step": 124
    },
    {
      "epoch": 0.38109756097560976,
      "grad_norm": 1.0964562892913818,
      "learning_rate": 2.9861442904921164e-06,
      "loss": 1.9498,
      "step": 125
    },
    {
      "epoch": 0.38414634146341464,
      "grad_norm": 0.9994953274726868,
      "learning_rate": 3.0100334448160537e-06,
      "loss": 1.9179,
      "step": 126
    },
    {
      "epoch": 0.3871951219512195,
      "grad_norm": 1.2992056608200073,
      "learning_rate": 3.0339225991399906e-06,
      "loss": 1.9501,
      "step": 127
    },
    {
      "epoch": 0.3902439024390244,
      "grad_norm": 1.2953836917877197,
      "learning_rate": 3.0578117534639275e-06,
      "loss": 1.9387,
      "step": 128
    },
    {
      "epoch": 0.3932926829268293,
      "grad_norm": 1.1477677822113037,
      "learning_rate": 3.0817009077878644e-06,
      "loss": 1.9368,
      "step": 129
    },
    {
      "epoch": 0.39634146341463417,
      "grad_norm": 1.1450083255767822,
      "learning_rate": 3.1055900621118013e-06,
      "loss": 1.9316,
      "step": 130
    },
    {
      "epoch": 0.39939024390243905,
      "grad_norm": 1.2700318098068237,
      "learning_rate": 3.1294792164357386e-06,
      "loss": 1.9545,
      "step": 131
    },
    {
      "epoch": 0.4024390243902439,
      "grad_norm": 1.3220826387405396,
      "learning_rate": 3.1533683707596755e-06,
      "loss": 1.9621,
      "step": 132
    },
    {
      "epoch": 0.4054878048780488,
      "grad_norm": 1.0700464248657227,
      "learning_rate": 3.1772575250836123e-06,
      "loss": 1.9432,
      "step": 133
    },
    {
      "epoch": 0.40853658536585363,
      "grad_norm": 1.1687523126602173,
      "learning_rate": 3.201146679407549e-06,
      "loss": 1.9629,
      "step": 134
    },
    {
      "epoch": 0.4115853658536585,
      "grad_norm": 1.203714370727539,
      "learning_rate": 3.2250358337314857e-06,
      "loss": 1.9422,
      "step": 135
    },
    {
      "epoch": 0.4146341463414634,
      "grad_norm": 1.3656145334243774,
      "learning_rate": 3.2489249880554234e-06,
      "loss": 1.9617,
      "step": 136
    },
    {
      "epoch": 0.4176829268292683,
      "grad_norm": 1.152235746383667,
      "learning_rate": 3.2728141423793603e-06,
      "loss": 1.9405,
      "step": 137
    },
    {
      "epoch": 0.42073170731707316,
      "grad_norm": 1.6484832763671875,
      "learning_rate": 3.2967032967032968e-06,
      "loss": 1.9237,
      "step": 138
    },
    {
      "epoch": 0.42378048780487804,
      "grad_norm": 0.869823694229126,
      "learning_rate": 3.3205924510272337e-06,
      "loss": 1.9301,
      "step": 139
    },
    {
      "epoch": 0.4268292682926829,
      "grad_norm": 1.455971598625183,
      "learning_rate": 3.3444816053511705e-06,
      "loss": 1.9295,
      "step": 140
    },
    {
      "epoch": 0.4298780487804878,
      "grad_norm": 1.0484917163848877,
      "learning_rate": 3.3683707596751074e-06,
      "loss": 1.9244,
      "step": 141
    },
    {
      "epoch": 0.4329268292682927,
      "grad_norm": 1.0218935012817383,
      "learning_rate": 3.3922599139990447e-06,
      "loss": 1.9218,
      "step": 142
    },
    {
      "epoch": 0.43597560975609756,
      "grad_norm": 1.1364078521728516,
      "learning_rate": 3.4161490683229816e-06,
      "loss": 1.9319,
      "step": 143
    },
    {
      "epoch": 0.43902439024390244,
      "grad_norm": 1.2479981184005737,
      "learning_rate": 3.4400382226469185e-06,
      "loss": 1.9215,
      "step": 144
    },
    {
      "epoch": 0.4420731707317073,
      "grad_norm": 1.084989070892334,
      "learning_rate": 3.4639273769708554e-06,
      "loss": 1.9458,
      "step": 145
    },
    {
      "epoch": 0.4451219512195122,
      "grad_norm": 1.1187472343444824,
      "learning_rate": 3.4878165312947923e-06,
      "loss": 1.9174,
      "step": 146
    },
    {
      "epoch": 0.4481707317073171,
      "grad_norm": 1.2813881635665894,
      "learning_rate": 3.511705685618729e-06,
      "loss": 1.9304,
      "step": 147
    },
    {
      "epoch": 0.45121951219512196,
      "grad_norm": 1.355372428894043,
      "learning_rate": 3.5355948399426665e-06,
      "loss": 1.9322,
      "step": 148
    },
    {
      "epoch": 0.45426829268292684,
      "grad_norm": 1.1824486255645752,
      "learning_rate": 3.5594839942666033e-06,
      "loss": 1.9262,
      "step": 149
    },
    {
      "epoch": 0.4573170731707317,
      "grad_norm": 1.0536565780639648,
      "learning_rate": 3.5833731485905402e-06,
      "loss": 1.9305,
      "step": 150
    },
    {
      "epoch": 0.4603658536585366,
      "grad_norm": 1.782344937324524,
      "learning_rate": 3.607262302914477e-06,
      "loss": 1.9579,
      "step": 151
    },
    {
      "epoch": 0.4634146341463415,
      "grad_norm": 1.2788569927215576,
      "learning_rate": 3.631151457238414e-06,
      "loss": 1.9147,
      "step": 152
    },
    {
      "epoch": 0.46646341463414637,
      "grad_norm": 1.0829970836639404,
      "learning_rate": 3.6550406115623505e-06,
      "loss": 1.9361,
      "step": 153
    },
    {
      "epoch": 0.4695121951219512,
      "grad_norm": 1.2328369617462158,
      "learning_rate": 3.678929765886288e-06,
      "loss": 1.9373,
      "step": 154
    },
    {
      "epoch": 0.4725609756097561,
      "grad_norm": 1.1621395349502563,
      "learning_rate": 3.702818920210225e-06,
      "loss": 1.9169,
      "step": 155
    },
    {
      "epoch": 0.47560975609756095,
      "grad_norm": 1.0509288311004639,
      "learning_rate": 3.726708074534162e-06,
      "loss": 1.9455,
      "step": 156
    },
    {
      "epoch": 0.47865853658536583,
      "grad_norm": 1.2185430526733398,
      "learning_rate": 3.7505972288580984e-06,
      "loss": 1.9297,
      "step": 157
    },
    {
      "epoch": 0.4817073170731707,
      "grad_norm": 1.222556233406067,
      "learning_rate": 3.7744863831820353e-06,
      "loss": 1.9442,
      "step": 158
    },
    {
      "epoch": 0.4847560975609756,
      "grad_norm": 1.3861175775527954,
      "learning_rate": 3.798375537505972e-06,
      "loss": 1.9516,
      "step": 159
    },
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 1.177702784538269,
      "learning_rate": 3.82226469182991e-06,
      "loss": 1.9354,
      "step": 160
    },
    {
      "epoch": 0.49085365853658536,
      "grad_norm": 1.5396702289581299,
      "learning_rate": 3.846153846153847e-06,
      "loss": 1.9277,
      "step": 161
    },
    {
      "epoch": 0.49390243902439024,
      "grad_norm": 1.6490375995635986,
      "learning_rate": 3.870043000477784e-06,
      "loss": 1.9366,
      "step": 162
    },
    {
      "epoch": 0.4969512195121951,
      "grad_norm": 1.3573803901672363,
      "learning_rate": 3.8939321548017206e-06,
      "loss": 1.9307,
      "step": 163
    },
    {
      "epoch": 0.5,
      "grad_norm": 1.3898566961288452,
      "learning_rate": 3.917821309125657e-06,
      "loss": 1.9151,
      "step": 164
    },
    {
      "epoch": 0.5030487804878049,
      "grad_norm": 1.2588611841201782,
      "learning_rate": 3.9417104634495935e-06,
      "loss": 1.9144,
      "step": 165
    },
    {
      "epoch": 0.5060975609756098,
      "grad_norm": 1.1295562982559204,
      "learning_rate": 3.965599617773531e-06,
      "loss": 1.9298,
      "step": 166
    },
    {
      "epoch": 0.5091463414634146,
      "grad_norm": 1.6052403450012207,
      "learning_rate": 3.989488772097468e-06,
      "loss": 1.9106,
      "step": 167
    },
    {
      "epoch": 0.5121951219512195,
      "grad_norm": 1.455314040184021,
      "learning_rate": 4.013377926421405e-06,
      "loss": 1.9148,
      "step": 168
    },
    {
      "epoch": 0.5152439024390244,
      "grad_norm": 1.221370816230774,
      "learning_rate": 4.037267080745342e-06,
      "loss": 1.9123,
      "step": 169
    },
    {
      "epoch": 0.5182926829268293,
      "grad_norm": 1.3300690650939941,
      "learning_rate": 4.061156235069279e-06,
      "loss": 1.9004,
      "step": 170
    },
    {
      "epoch": 0.5213414634146342,
      "grad_norm": 1.5324848890304565,
      "learning_rate": 4.085045389393216e-06,
      "loss": 1.9269,
      "step": 171
    },
    {
      "epoch": 0.524390243902439,
      "grad_norm": 1.7419891357421875,
      "learning_rate": 4.108934543717153e-06,
      "loss": 1.9058,
      "step": 172
    },
    {
      "epoch": 0.5274390243902439,
      "grad_norm": 1.110393762588501,
      "learning_rate": 4.132823698041089e-06,
      "loss": 1.9101,
      "step": 173
    },
    {
      "epoch": 0.5304878048780488,
      "grad_norm": 1.276220440864563,
      "learning_rate": 4.156712852365026e-06,
      "loss": 1.951,
      "step": 174
    },
    {
      "epoch": 0.5335365853658537,
      "grad_norm": 1.0034399032592773,
      "learning_rate": 4.180602006688963e-06,
      "loss": 1.916,
      "step": 175
    },
    {
      "epoch": 0.5365853658536586,
      "grad_norm": 1.4551115036010742,
      "learning_rate": 4.2044911610129e-06,
      "loss": 1.9527,
      "step": 176
    },
    {
      "epoch": 0.5396341463414634,
      "grad_norm": 1.1432304382324219,
      "learning_rate": 4.228380315336837e-06,
      "loss": 1.9417,
      "step": 177
    },
    {
      "epoch": 0.5426829268292683,
      "grad_norm": 1.3251712322235107,
      "learning_rate": 4.252269469660775e-06,
      "loss": 1.9389,
      "step": 178
    },
    {
      "epoch": 0.5457317073170732,
      "grad_norm": 1.2964903116226196,
      "learning_rate": 4.2761586239847116e-06,
      "loss": 1.9276,
      "step": 179
    },
    {
      "epoch": 0.5487804878048781,
      "grad_norm": 1.4290186166763306,
      "learning_rate": 4.3000477783086484e-06,
      "loss": 1.8939,
      "step": 180
    },
    {
      "epoch": 0.551829268292683,
      "grad_norm": 1.4190891981124878,
      "learning_rate": 4.323936932632585e-06,
      "loss": 1.9231,
      "step": 181
    },
    {
      "epoch": 0.5548780487804879,
      "grad_norm": 0.9606363773345947,
      "learning_rate": 4.347826086956522e-06,
      "loss": 1.9193,
      "step": 182
    },
    {
      "epoch": 0.5579268292682927,
      "grad_norm": 1.2942814826965332,
      "learning_rate": 4.371715241280458e-06,
      "loss": 1.9284,
      "step": 183
    },
    {
      "epoch": 0.5609756097560976,
      "grad_norm": 1.4709460735321045,
      "learning_rate": 4.395604395604396e-06,
      "loss": 1.9509,
      "step": 184
    },
    {
      "epoch": 0.5640243902439024,
      "grad_norm": 1.2656466960906982,
      "learning_rate": 4.419493549928333e-06,
      "loss": 1.8799,
      "step": 185
    },
    {
      "epoch": 0.5670731707317073,
      "grad_norm": 1.399861216545105,
      "learning_rate": 4.44338270425227e-06,
      "loss": 1.9156,
      "step": 186
    },
    {
      "epoch": 0.5701219512195121,
      "grad_norm": 1.3140912055969238,
      "learning_rate": 4.467271858576207e-06,
      "loss": 1.9319,
      "step": 187
    },
    {
      "epoch": 0.573170731707317,
      "grad_norm": 1.2447776794433594,
      "learning_rate": 4.4911610129001435e-06,
      "loss": 1.9137,
      "step": 188
    },
    {
      "epoch": 0.5762195121951219,
      "grad_norm": 1.3533416986465454,
      "learning_rate": 4.51505016722408e-06,
      "loss": 1.9388,
      "step": 189
    },
    {
      "epoch": 0.5792682926829268,
      "grad_norm": 1.1682474613189697,
      "learning_rate": 4.538939321548018e-06,
      "loss": 1.936,
      "step": 190
    },
    {
      "epoch": 0.5823170731707317,
      "grad_norm": 1.0747292041778564,
      "learning_rate": 4.562828475871954e-06,
      "loss": 1.9368,
      "step": 191
    },
    {
      "epoch": 0.5853658536585366,
      "grad_norm": 1.0626906156539917,
      "learning_rate": 4.586717630195891e-06,
      "loss": 1.943,
      "step": 192
    },
    {
      "epoch": 0.5884146341463414,
      "grad_norm": 0.9980499148368835,
      "learning_rate": 4.610606784519828e-06,
      "loss": 1.9351,
      "step": 193
    },
    {
      "epoch": 0.5914634146341463,
      "grad_norm": 1.2597066164016724,
      "learning_rate": 4.634495938843765e-06,
      "loss": 1.9276,
      "step": 194
    },
    {
      "epoch": 0.5945121951219512,
      "grad_norm": 0.90044105052948,
      "learning_rate": 4.658385093167702e-06,
      "loss": 1.9225,
      "step": 195
    },
    {
      "epoch": 0.5975609756097561,
      "grad_norm": 1.7012243270874023,
      "learning_rate": 4.682274247491639e-06,
      "loss": 1.9068,
      "step": 196
    },
    {
      "epoch": 0.600609756097561,
      "grad_norm": 1.2246321439743042,
      "learning_rate": 4.706163401815576e-06,
      "loss": 1.9021,
      "step": 197
    },
    {
      "epoch": 0.6036585365853658,
      "grad_norm": 1.7489140033721924,
      "learning_rate": 4.730052556139513e-06,
      "loss": 1.9087,
      "step": 198
    },
    {
      "epoch": 0.6067073170731707,
      "grad_norm": 1.0586602687835693,
      "learning_rate": 4.75394171046345e-06,
      "loss": 1.9161,
      "step": 199
    },
    {
      "epoch": 0.6097560975609756,
      "grad_norm": 1.2212610244750977,
      "learning_rate": 4.777830864787387e-06,
      "loss": 1.924,
      "step": 200
    },
    {
      "epoch": 0.6128048780487805,
      "grad_norm": 1.3265150785446167,
      "learning_rate": 4.801720019111324e-06,
      "loss": 1.8907,
      "step": 201
    },
    {
      "epoch": 0.6158536585365854,
      "grad_norm": 1.007858157157898,
      "learning_rate": 4.82560917343526e-06,
      "loss": 1.9133,
      "step": 202
    },
    {
      "epoch": 0.6189024390243902,
      "grad_norm": 1.4028000831604004,
      "learning_rate": 4.849498327759198e-06,
      "loss": 1.9252,
      "step": 203
    },
    {
      "epoch": 0.6219512195121951,
      "grad_norm": 1.5757172107696533,
      "learning_rate": 4.8733874820831345e-06,
      "loss": 1.9556,
      "step": 204
    },
    {
      "epoch": 0.625,
      "grad_norm": 1.7050843238830566,
      "learning_rate": 4.897276636407071e-06,
      "loss": 1.9286,
      "step": 205
    },
    {
      "epoch": 0.6280487804878049,
      "grad_norm": 1.480800986289978,
      "learning_rate": 4.921165790731008e-06,
      "loss": 1.9374,
      "step": 206
    },
    {
      "epoch": 0.6310975609756098,
      "grad_norm": 1.3798495531082153,
      "learning_rate": 4.945054945054945e-06,
      "loss": 1.9324,
      "step": 207
    },
    {
      "epoch": 0.6341463414634146,
      "grad_norm": 1.2658337354660034,
      "learning_rate": 4.968944099378882e-06,
      "loss": 1.9412,
      "step": 208
    },
    {
      "epoch": 0.6371951219512195,
      "grad_norm": 1.3130714893341064,
      "learning_rate": 4.99283325370282e-06,
      "loss": 1.9335,
      "step": 209
    },
    {
      "epoch": 0.6402439024390244,
      "grad_norm": 1.5272904634475708,
      "learning_rate": 5.016722408026756e-06,
      "loss": 1.9063,
      "step": 210
    },
    {
      "epoch": 0.6432926829268293,
      "grad_norm": 1.5313810110092163,
      "learning_rate": 5.040611562350693e-06,
      "loss": 1.9213,
      "step": 211
    },
    {
      "epoch": 0.6463414634146342,
      "grad_norm": 1.5184693336486816,
      "learning_rate": 5.0645007166746296e-06,
      "loss": 1.944,
      "step": 212
    },
    {
      "epoch": 0.649390243902439,
      "grad_norm": 1.305742621421814,
      "learning_rate": 5.0883898709985665e-06,
      "loss": 1.9485,
      "step": 213
    },
    {
      "epoch": 0.6524390243902439,
      "grad_norm": 1.8942409753799438,
      "learning_rate": 5.112279025322503e-06,
      "loss": 1.9511,
      "step": 214
    },
    {
      "epoch": 0.6554878048780488,
      "grad_norm": 1.1546876430511475,
      "learning_rate": 5.136168179646441e-06,
      "loss": 1.9329,
      "step": 215
    },
    {
      "epoch": 0.6585365853658537,
      "grad_norm": 1.5213189125061035,
      "learning_rate": 5.160057333970378e-06,
      "loss": 1.9145,
      "step": 216
    },
    {
      "epoch": 0.6615853658536586,
      "grad_norm": 1.2611969709396362,
      "learning_rate": 5.183946488294315e-06,
      "loss": 1.9195,
      "step": 217
    },
    {
      "epoch": 0.6646341463414634,
      "grad_norm": 1.3358227014541626,
      "learning_rate": 5.207835642618252e-06,
      "loss": 1.8946,
      "step": 218
    },
    {
      "epoch": 0.6676829268292683,
      "grad_norm": 1.637919545173645,
      "learning_rate": 5.231724796942189e-06,
      "loss": 1.9252,
      "step": 219
    },
    {
      "epoch": 0.6707317073170732,
      "grad_norm": 1.1602742671966553,
      "learning_rate": 5.2556139512661255e-06,
      "loss": 1.8923,
      "step": 220
    },
    {
      "epoch": 0.6737804878048781,
      "grad_norm": 0.9740343689918518,
      "learning_rate": 5.279503105590062e-06,
      "loss": 1.9194,
      "step": 221
    },
    {
      "epoch": 0.676829268292683,
      "grad_norm": 1.4076448678970337,
      "learning_rate": 5.303392259913999e-06,
      "loss": 1.9264,
      "step": 222
    },
    {
      "epoch": 0.6798780487804879,
      "grad_norm": 1.3798861503601074,
      "learning_rate": 5.327281414237936e-06,
      "loss": 1.9212,
      "step": 223
    },
    {
      "epoch": 0.6829268292682927,
      "grad_norm": 1.2205013036727905,
      "learning_rate": 5.351170568561873e-06,
      "loss": 1.9051,
      "step": 224
    },
    {
      "epoch": 0.6859756097560976,
      "grad_norm": 1.0574887990951538,
      "learning_rate": 5.37505972288581e-06,
      "loss": 1.9168,
      "step": 225
    },
    {
      "epoch": 0.6890243902439024,
      "grad_norm": 1.3339234590530396,
      "learning_rate": 5.398948877209747e-06,
      "loss": 1.8921,
      "step": 226
    },
    {
      "epoch": 0.6920731707317073,
      "grad_norm": 1.111090064048767,
      "learning_rate": 5.4228380315336845e-06,
      "loss": 1.8952,
      "step": 227
    },
    {
      "epoch": 0.6951219512195121,
      "grad_norm": 1.4310805797576904,
      "learning_rate": 5.446727185857621e-06,
      "loss": 1.9149,
      "step": 228
    },
    {
      "epoch": 0.698170731707317,
      "grad_norm": 1.6895301342010498,
      "learning_rate": 5.4706163401815574e-06,
      "loss": 1.948,
      "step": 229
    },
    {
      "epoch": 0.7012195121951219,
      "grad_norm": 2.024019241333008,
      "learning_rate": 5.494505494505494e-06,
      "loss": 1.9095,
      "step": 230
    },
    {
      "epoch": 0.7042682926829268,
      "grad_norm": 1.2090539932250977,
      "learning_rate": 5.518394648829431e-06,
      "loss": 1.9139,
      "step": 231
    },
    {
      "epoch": 0.7073170731707317,
      "grad_norm": 1.0041208267211914,
      "learning_rate": 5.542283803153368e-06,
      "loss": 1.9078,
      "step": 232
    },
    {
      "epoch": 0.7103658536585366,
      "grad_norm": 1.1134798526763916,
      "learning_rate": 5.566172957477306e-06,
      "loss": 1.9233,
      "step": 233
    },
    {
      "epoch": 0.7134146341463414,
      "grad_norm": 1.3305740356445312,
      "learning_rate": 5.590062111801243e-06,
      "loss": 1.9464,
      "step": 234
    },
    {
      "epoch": 0.7164634146341463,
      "grad_norm": 1.243301510810852,
      "learning_rate": 5.61395126612518e-06,
      "loss": 1.9081,
      "step": 235
    },
    {
      "epoch": 0.7195121951219512,
      "grad_norm": 0.9772629141807556,
      "learning_rate": 5.6378404204491165e-06,
      "loss": 1.906,
      "step": 236
    },
    {
      "epoch": 0.7225609756097561,
      "grad_norm": 1.1151772737503052,
      "learning_rate": 5.661729574773053e-06,
      "loss": 1.8856,
      "step": 237
    },
    {
      "epoch": 0.725609756097561,
      "grad_norm": 1.019137978553772,
      "learning_rate": 5.68561872909699e-06,
      "loss": 1.9219,
      "step": 238
    },
    {
      "epoch": 0.7286585365853658,
      "grad_norm": 1.1912275552749634,
      "learning_rate": 5.709507883420927e-06,
      "loss": 1.9038,
      "step": 239
    },
    {
      "epoch": 0.7317073170731707,
      "grad_norm": 1.2323520183563232,
      "learning_rate": 5.733397037744864e-06,
      "loss": 1.9263,
      "step": 240
    },
    {
      "epoch": 0.7347560975609756,
      "grad_norm": 1.2200489044189453,
      "learning_rate": 5.757286192068801e-06,
      "loss": 1.9144,
      "step": 241
    },
    {
      "epoch": 0.7378048780487805,
      "grad_norm": 1.5248363018035889,
      "learning_rate": 5.781175346392738e-06,
      "loss": 1.9369,
      "step": 242
    },
    {
      "epoch": 0.7408536585365854,
      "grad_norm": 1.0916991233825684,
      "learning_rate": 5.805064500716675e-06,
      "loss": 1.8836,
      "step": 243
    },
    {
      "epoch": 0.7439024390243902,
      "grad_norm": 1.0971890687942505,
      "learning_rate": 5.8289536550406116e-06,
      "loss": 1.9299,
      "step": 244
    },
    {
      "epoch": 0.7469512195121951,
      "grad_norm": 1.1186984777450562,
      "learning_rate": 5.852842809364549e-06,
      "loss": 1.9036,
      "step": 245
    },
    {
      "epoch": 0.75,
      "grad_norm": 1.3337066173553467,
      "learning_rate": 5.876731963688486e-06,
      "loss": 1.9098,
      "step": 246
    },
    {
      "epoch": 0.7530487804878049,
      "grad_norm": 1.252734661102295,
      "learning_rate": 5.900621118012423e-06,
      "loss": 1.9281,
      "step": 247
    },
    {
      "epoch": 0.7560975609756098,
      "grad_norm": 1.2734955549240112,
      "learning_rate": 5.924510272336359e-06,
      "loss": 1.9299,
      "step": 248
    },
    {
      "epoch": 0.7591463414634146,
      "grad_norm": 0.9368515014648438,
      "learning_rate": 5.948399426660296e-06,
      "loss": 1.9143,
      "step": 249
    },
    {
      "epoch": 0.7621951219512195,
      "grad_norm": 1.0508118867874146,
      "learning_rate": 5.972288580984233e-06,
      "loss": 1.9146,
      "step": 250
    },
    {
      "epoch": 0.7652439024390244,
      "grad_norm": 1.3169258832931519,
      "learning_rate": 5.996177735308171e-06,
      "loss": 1.8951,
      "step": 251
    },
    {
      "epoch": 0.7682926829268293,
      "grad_norm": 1.4720158576965332,
      "learning_rate": 6.0200668896321075e-06,
      "loss": 1.9096,
      "step": 252
    },
    {
      "epoch": 0.7713414634146342,
      "grad_norm": 0.7764363884925842,
      "learning_rate": 6.043956043956044e-06,
      "loss": 1.9055,
      "step": 253
    },
    {
      "epoch": 0.774390243902439,
      "grad_norm": 1.5512456893920898,
      "learning_rate": 6.067845198279981e-06,
      "loss": 1.9237,
      "step": 254
    },
    {
      "epoch": 0.7774390243902439,
      "grad_norm": 1.321635365486145,
      "learning_rate": 6.091734352603918e-06,
      "loss": 1.8904,
      "step": 255
    },
    {
      "epoch": 0.7804878048780488,
      "grad_norm": 1.0005732774734497,
      "learning_rate": 6.115623506927855e-06,
      "loss": 1.8974,
      "step": 256
    },
    {
      "epoch": 0.7835365853658537,
      "grad_norm": 1.8487420082092285,
      "learning_rate": 6.139512661251792e-06,
      "loss": 1.9189,
      "step": 257
    },
    {
      "epoch": 0.7865853658536586,
      "grad_norm": 1.1215136051177979,
      "learning_rate": 6.163401815575729e-06,
      "loss": 1.8998,
      "step": 258
    },
    {
      "epoch": 0.7896341463414634,
      "grad_norm": 1.438992977142334,
      "learning_rate": 6.187290969899666e-06,
      "loss": 1.9019,
      "step": 259
    },
    {
      "epoch": 0.7926829268292683,
      "grad_norm": 1.183056354522705,
      "learning_rate": 6.2111801242236025e-06,
      "loss": 1.921,
      "step": 260
    },
    {
      "epoch": 0.7957317073170732,
      "grad_norm": 1.1750491857528687,
      "learning_rate": 6.2350692785475394e-06,
      "loss": 1.9189,
      "step": 261
    },
    {
      "epoch": 0.7987804878048781,
      "grad_norm": 1.2113746404647827,
      "learning_rate": 6.258958432871477e-06,
      "loss": 1.9053,
      "step": 262
    },
    {
      "epoch": 0.801829268292683,
      "grad_norm": 1.0733524560928345,
      "learning_rate": 6.282847587195413e-06,
      "loss": 1.9128,
      "step": 263
    },
    {
      "epoch": 0.8048780487804879,
      "grad_norm": 1.0429428815841675,
      "learning_rate": 6.306736741519351e-06,
      "loss": 1.8912,
      "step": 264
    },
    {
      "epoch": 0.8079268292682927,
      "grad_norm": 1.1305841207504272,
      "learning_rate": 6.330625895843287e-06,
      "loss": 1.9158,
      "step": 265
    },
    {
      "epoch": 0.8109756097560976,
      "grad_norm": 1.7175166606903076,
      "learning_rate": 6.354515050167225e-06,
      "loss": 1.9299,
      "step": 266
    },
    {
      "epoch": 0.8140243902439024,
      "grad_norm": 1.5184255838394165,
      "learning_rate": 6.378404204491162e-06,
      "loss": 1.8849,
      "step": 267
    },
    {
      "epoch": 0.8170731707317073,
      "grad_norm": 1.190453052520752,
      "learning_rate": 6.402293358815098e-06,
      "loss": 1.8783,
      "step": 268
    },
    {
      "epoch": 0.8201219512195121,
      "grad_norm": 1.28657865524292,
      "learning_rate": 6.426182513139035e-06,
      "loss": 1.9053,
      "step": 269
    },
    {
      "epoch": 0.823170731707317,
      "grad_norm": 1.048496127128601,
      "learning_rate": 6.450071667462971e-06,
      "loss": 1.9011,
      "step": 270
    },
    {
      "epoch": 0.8262195121951219,
      "grad_norm": 1.3134204149246216,
      "learning_rate": 6.473960821786909e-06,
      "loss": 1.892,
      "step": 271
    },
    {
      "epoch": 0.8292682926829268,
      "grad_norm": 0.9655436873435974,
      "learning_rate": 6.497849976110847e-06,
      "loss": 1.9038,
      "step": 272
    },
    {
      "epoch": 0.8323170731707317,
      "grad_norm": 1.2910897731781006,
      "learning_rate": 6.521739130434783e-06,
      "loss": 1.9027,
      "step": 273
    },
    {
      "epoch": 0.8353658536585366,
      "grad_norm": 1.3932114839553833,
      "learning_rate": 6.545628284758721e-06,
      "loss": 1.8859,
      "step": 274
    },
    {
      "epoch": 0.8384146341463414,
      "grad_norm": 1.0009571313858032,
      "learning_rate": 6.569517439082657e-06,
      "loss": 1.9085,
      "step": 275
    },
    {
      "epoch": 0.8414634146341463,
      "grad_norm": 1.7092267274856567,
      "learning_rate": 6.5934065934065935e-06,
      "loss": 1.9029,
      "step": 276
    },
    {
      "epoch": 0.8445121951219512,
      "grad_norm": 1.1357864141464233,
      "learning_rate": 6.61729574773053e-06,
      "loss": 1.8981,
      "step": 277
    },
    {
      "epoch": 0.8475609756097561,
      "grad_norm": 0.9810603857040405,
      "learning_rate": 6.641184902054467e-06,
      "loss": 1.9068,
      "step": 278
    },
    {
      "epoch": 0.850609756097561,
      "grad_norm": 1.423130750656128,
      "learning_rate": 6.665074056378405e-06,
      "loss": 1.906,
      "step": 279
    },
    {
      "epoch": 0.8536585365853658,
      "grad_norm": 1.0767755508422852,
      "learning_rate": 6.688963210702341e-06,
      "loss": 1.8626,
      "step": 280
    },
    {
      "epoch": 0.8567073170731707,
      "grad_norm": 1.1725006103515625,
      "learning_rate": 6.712852365026279e-06,
      "loss": 1.9086,
      "step": 281
    },
    {
      "epoch": 0.8597560975609756,
      "grad_norm": 1.5360018014907837,
      "learning_rate": 6.736741519350215e-06,
      "loss": 1.9257,
      "step": 282
    },
    {
      "epoch": 0.8628048780487805,
      "grad_norm": 1.2383347749710083,
      "learning_rate": 6.7606306736741526e-06,
      "loss": 1.881,
      "step": 283
    },
    {
      "epoch": 0.8658536585365854,
      "grad_norm": 1.4837791919708252,
      "learning_rate": 6.7845198279980895e-06,
      "loss": 1.8943,
      "step": 284
    },
    {
      "epoch": 0.8689024390243902,
      "grad_norm": 1.0741833448410034,
      "learning_rate": 6.808408982322026e-06,
      "loss": 1.8918,
      "step": 285
    },
    {
      "epoch": 0.8719512195121951,
      "grad_norm": 1.3302024602890015,
      "learning_rate": 6.832298136645963e-06,
      "loss": 1.9013,
      "step": 286
    },
    {
      "epoch": 0.875,
      "grad_norm": 1.2674635648727417,
      "learning_rate": 6.856187290969899e-06,
      "loss": 1.8999,
      "step": 287
    },
    {
      "epoch": 0.8780487804878049,
      "grad_norm": 0.9276521801948547,
      "learning_rate": 6.880076445293837e-06,
      "loss": 1.893,
      "step": 288
    },
    {
      "epoch": 0.8810975609756098,
      "grad_norm": 1.4239391088485718,
      "learning_rate": 6.903965599617773e-06,
      "loss": 1.8887,
      "step": 289
    },
    {
      "epoch": 0.8841463414634146,
      "grad_norm": 1.3026734590530396,
      "learning_rate": 6.927854753941711e-06,
      "loss": 1.8857,
      "step": 290
    },
    {
      "epoch": 0.8871951219512195,
      "grad_norm": 1.2964493036270142,
      "learning_rate": 6.9517439082656485e-06,
      "loss": 1.8859,
      "step": 291
    },
    {
      "epoch": 0.8902439024390244,
      "grad_norm": 1.339819312095642,
      "learning_rate": 6.9756330625895845e-06,
      "loss": 1.8876,
      "step": 292
    },
    {
      "epoch": 0.8932926829268293,
      "grad_norm": 0.8370761871337891,
      "learning_rate": 6.999522216913522e-06,
      "loss": 1.8819,
      "step": 293
    },
    {
      "epoch": 0.8963414634146342,
      "grad_norm": 1.6998356580734253,
      "learning_rate": 7.023411371237458e-06,
      "loss": 1.9235,
      "step": 294
    },
    {
      "epoch": 0.899390243902439,
      "grad_norm": 1.454134225845337,
      "learning_rate": 7.047300525561395e-06,
      "loss": 1.8802,
      "step": 295
    },
    {
      "epoch": 0.9024390243902439,
      "grad_norm": 1.5796455144882202,
      "learning_rate": 7.071189679885333e-06,
      "loss": 1.8845,
      "step": 296
    },
    {
      "epoch": 0.9054878048780488,
      "grad_norm": 1.4182636737823486,
      "learning_rate": 7.095078834209269e-06,
      "loss": 1.9092,
      "step": 297
    },
    {
      "epoch": 0.9085365853658537,
      "grad_norm": 1.569122552871704,
      "learning_rate": 7.118967988533207e-06,
      "loss": 1.8991,
      "step": 298
    },
    {
      "epoch": 0.9115853658536586,
      "grad_norm": 1.0907398462295532,
      "learning_rate": 7.142857142857143e-06,
      "loss": 1.8939,
      "step": 299
    },
    {
      "epoch": 0.9146341463414634,
      "grad_norm": 1.0461949110031128,
      "learning_rate": 7.1667462971810804e-06,
      "loss": 1.8853,
      "step": 300
    },
    {
      "epoch": 0.9176829268292683,
      "grad_norm": 1.1864503622055054,
      "learning_rate": 7.1906354515050165e-06,
      "loss": 1.8943,
      "step": 301
    },
    {
      "epoch": 0.9207317073170732,
      "grad_norm": 1.5050891637802124,
      "learning_rate": 7.214524605828954e-06,
      "loss": 1.8778,
      "step": 302
    },
    {
      "epoch": 0.9237804878048781,
      "grad_norm": 1.042526364326477,
      "learning_rate": 7.238413760152891e-06,
      "loss": 1.8943,
      "step": 303
    },
    {
      "epoch": 0.926829268292683,
      "grad_norm": 1.113753318786621,
      "learning_rate": 7.262302914476828e-06,
      "loss": 1.8764,
      "step": 304
    },
    {
      "epoch": 0.9298780487804879,
      "grad_norm": 1.1224074363708496,
      "learning_rate": 7.286192068800765e-06,
      "loss": 1.882,
      "step": 305
    },
    {
      "epoch": 0.9329268292682927,
      "grad_norm": 1.3235355615615845,
      "learning_rate": 7.310081223124701e-06,
      "loss": 1.9187,
      "step": 306
    },
    {
      "epoch": 0.9359756097560976,
      "grad_norm": 1.1772518157958984,
      "learning_rate": 7.333970377448639e-06,
      "loss": 1.8648,
      "step": 307
    },
    {
      "epoch": 0.9390243902439024,
      "grad_norm": 0.9901207089424133,
      "learning_rate": 7.357859531772576e-06,
      "loss": 1.8921,
      "step": 308
    },
    {
      "epoch": 0.9420731707317073,
      "grad_norm": 1.5677798986434937,
      "learning_rate": 7.381748686096512e-06,
      "loss": 1.8766,
      "step": 309
    },
    {
      "epoch": 0.9451219512195121,
      "grad_norm": 1.306951642036438,
      "learning_rate": 7.40563784042045e-06,
      "loss": 1.906,
      "step": 310
    },
    {
      "epoch": 0.948170731707317,
      "grad_norm": 1.6262617111206055,
      "learning_rate": 7.429526994744386e-06,
      "loss": 1.8828,
      "step": 311
    },
    {
      "epoch": 0.9512195121951219,
      "grad_norm": 1.1887810230255127,
      "learning_rate": 7.453416149068324e-06,
      "loss": 1.8818,
      "step": 312
    },
    {
      "epoch": 0.9542682926829268,
      "grad_norm": 1.2932298183441162,
      "learning_rate": 7.47730530339226e-06,
      "loss": 1.9116,
      "step": 313
    },
    {
      "epoch": 0.9573170731707317,
      "grad_norm": 1.4462857246398926,
      "learning_rate": 7.501194457716197e-06,
      "loss": 1.8905,
      "step": 314
    },
    {
      "epoch": 0.9603658536585366,
      "grad_norm": 0.9686806797981262,
      "learning_rate": 7.5250836120401346e-06,
      "loss": 1.8872,
      "step": 315
    },
    {
      "epoch": 0.9634146341463414,
      "grad_norm": 1.505776286125183,
      "learning_rate": 7.548972766364071e-06,
      "loss": 1.8732,
      "step": 316
    },
    {
      "epoch": 0.9664634146341463,
      "grad_norm": 1.3193867206573486,
      "learning_rate": 7.572861920688008e-06,
      "loss": 1.8779,
      "step": 317
    },
    {
      "epoch": 0.9695121951219512,
      "grad_norm": 1.2116954326629639,
      "learning_rate": 7.596751075011944e-06,
      "loss": 1.8616,
      "step": 318
    },
    {
      "epoch": 0.9725609756097561,
      "grad_norm": 1.2312524318695068,
      "learning_rate": 7.620640229335882e-06,
      "loss": 1.8716,
      "step": 319
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 1.491804838180542,
      "learning_rate": 7.64452938365982e-06,
      "loss": 1.8884,
      "step": 320
    },
    {
      "epoch": 0.9786585365853658,
      "grad_norm": 1.4990068674087524,
      "learning_rate": 7.668418537983756e-06,
      "loss": 1.8668,
      "step": 321
    },
    {
      "epoch": 0.9817073170731707,
      "grad_norm": 1.259024977684021,
      "learning_rate": 7.692307692307694e-06,
      "loss": 1.893,
      "step": 322
    },
    {
      "epoch": 0.9847560975609756,
      "grad_norm": 1.2142138481140137,
      "learning_rate": 7.71619684663163e-06,
      "loss": 1.8928,
      "step": 323
    },
    {
      "epoch": 0.9878048780487805,
      "grad_norm": 1.4802168607711792,
      "learning_rate": 7.740086000955567e-06,
      "loss": 1.8729,
      "step": 324
    },
    {
      "epoch": 0.9908536585365854,
      "grad_norm": 1.6211334466934204,
      "learning_rate": 7.763975155279503e-06,
      "loss": 1.8741,
      "step": 325
    },
    {
      "epoch": 0.9939024390243902,
      "grad_norm": 1.4747838973999023,
      "learning_rate": 7.787864309603441e-06,
      "loss": 1.863,
      "step": 326
    },
    {
      "epoch": 0.9969512195121951,
      "grad_norm": 1.1264011859893799,
      "learning_rate": 7.811753463927377e-06,
      "loss": 1.8874,
      "step": 327
    },
    {
      "epoch": 1.0,
      "grad_norm": 3.634932279586792,
      "learning_rate": 7.835642618251313e-06,
      "loss": 1.8614,
      "step": 328
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.5139472678639664,
      "eval_loss": 1.8722717761993408,
      "eval_model_preparation_time": 0.0015,
      "eval_runtime": 3.2776,
      "eval_samples_per_second": 798.454,
      "eval_steps_per_second": 25.018,
      "step": 328
    },
    {
      "epoch": 1.0030487804878048,
      "grad_norm": 1.192173719406128,
      "learning_rate": 7.859531772575251e-06,
      "loss": 1.8843,
      "step": 329
    },
    {
      "epoch": 1.0060975609756098,
      "grad_norm": 1.649633765220642,
      "learning_rate": 7.883420926899187e-06,
      "loss": 1.8772,
      "step": 330
    },
    {
      "epoch": 1.0091463414634145,
      "grad_norm": 1.4703125953674316,
      "learning_rate": 7.907310081223125e-06,
      "loss": 1.8791,
      "step": 331
    },
    {
      "epoch": 1.0121951219512195,
      "grad_norm": 1.3562366962432861,
      "learning_rate": 7.931199235547062e-06,
      "loss": 1.8876,
      "step": 332
    },
    {
      "epoch": 1.0152439024390243,
      "grad_norm": 1.482030987739563,
      "learning_rate": 7.955088389870998e-06,
      "loss": 1.8658,
      "step": 333
    },
    {
      "epoch": 1.0182926829268293,
      "grad_norm": 1.2101227045059204,
      "learning_rate": 7.978977544194936e-06,
      "loss": 1.8634,
      "step": 334
    },
    {
      "epoch": 1.021341463414634,
      "grad_norm": 1.0142064094543457,
      "learning_rate": 8.002866698518872e-06,
      "loss": 1.879,
      "step": 335
    },
    {
      "epoch": 1.024390243902439,
      "grad_norm": 1.3145740032196045,
      "learning_rate": 8.02675585284281e-06,
      "loss": 1.8805,
      "step": 336
    },
    {
      "epoch": 1.0274390243902438,
      "grad_norm": 1.6096161603927612,
      "learning_rate": 8.050645007166746e-06,
      "loss": 1.8927,
      "step": 337
    },
    {
      "epoch": 1.0304878048780488,
      "grad_norm": 1.5552805662155151,
      "learning_rate": 8.074534161490684e-06,
      "loss": 1.8818,
      "step": 338
    },
    {
      "epoch": 1.0335365853658536,
      "grad_norm": 1.0740110874176025,
      "learning_rate": 8.098423315814621e-06,
      "loss": 1.8865,
      "step": 339
    },
    {
      "epoch": 1.0365853658536586,
      "grad_norm": 1.2917156219482422,
      "learning_rate": 8.122312470138558e-06,
      "loss": 1.87,
      "step": 340
    },
    {
      "epoch": 1.0396341463414633,
      "grad_norm": 1.440449595451355,
      "learning_rate": 8.146201624462495e-06,
      "loss": 1.8667,
      "step": 341
    },
    {
      "epoch": 1.0426829268292683,
      "grad_norm": 1.113442301750183,
      "learning_rate": 8.170090778786431e-06,
      "loss": 1.8755,
      "step": 342
    },
    {
      "epoch": 1.045731707317073,
      "grad_norm": 1.1321239471435547,
      "learning_rate": 8.193979933110369e-06,
      "loss": 1.8866,
      "step": 343
    },
    {
      "epoch": 1.048780487804878,
      "grad_norm": 1.939239501953125,
      "learning_rate": 8.217869087434307e-06,
      "loss": 1.8504,
      "step": 344
    },
    {
      "epoch": 1.0518292682926829,
      "grad_norm": 1.674332857131958,
      "learning_rate": 8.241758241758243e-06,
      "loss": 1.9004,
      "step": 345
    },
    {
      "epoch": 1.0548780487804879,
      "grad_norm": 1.4494179487228394,
      "learning_rate": 8.265647396082179e-06,
      "loss": 1.8651,
      "step": 346
    },
    {
      "epoch": 1.0579268292682926,
      "grad_norm": 1.0574675798416138,
      "learning_rate": 8.289536550406115e-06,
      "loss": 1.8605,
      "step": 347
    },
    {
      "epoch": 1.0609756097560976,
      "grad_norm": 1.2340019941329956,
      "learning_rate": 8.313425704730053e-06,
      "loss": 1.907,
      "step": 348
    },
    {
      "epoch": 1.0640243902439024,
      "grad_norm": 1.4489085674285889,
      "learning_rate": 8.337314859053989e-06,
      "loss": 1.8429,
      "step": 349
    },
    {
      "epoch": 1.0670731707317074,
      "grad_norm": 1.4047107696533203,
      "learning_rate": 8.361204013377926e-06,
      "loss": 1.8967,
      "step": 350
    },
    {
      "epoch": 1.0701219512195121,
      "grad_norm": 1.0401055812835693,
      "learning_rate": 8.385093167701864e-06,
      "loss": 1.8571,
      "step": 351
    },
    {
      "epoch": 1.0731707317073171,
      "grad_norm": 1.3917573690414429,
      "learning_rate": 8.4089823220258e-06,
      "loss": 1.8579,
      "step": 352
    },
    {
      "epoch": 1.076219512195122,
      "grad_norm": 1.52467679977417,
      "learning_rate": 8.432871476349738e-06,
      "loss": 1.8535,
      "step": 353
    },
    {
      "epoch": 1.079268292682927,
      "grad_norm": 1.4344792366027832,
      "learning_rate": 8.456760630673674e-06,
      "loss": 1.8892,
      "step": 354
    },
    {
      "epoch": 1.0823170731707317,
      "grad_norm": 1.3259212970733643,
      "learning_rate": 8.480649784997612e-06,
      "loss": 1.8502,
      "step": 355
    },
    {
      "epoch": 1.0853658536585367,
      "grad_norm": 1.6813569068908691,
      "learning_rate": 8.50453893932155e-06,
      "loss": 1.8923,
      "step": 356
    },
    {
      "epoch": 1.0884146341463414,
      "grad_norm": 1.2649824619293213,
      "learning_rate": 8.528428093645485e-06,
      "loss": 1.863,
      "step": 357
    },
    {
      "epoch": 1.0914634146341464,
      "grad_norm": 1.3776799440383911,
      "learning_rate": 8.552317247969423e-06,
      "loss": 1.8842,
      "step": 358
    },
    {
      "epoch": 1.0945121951219512,
      "grad_norm": 1.4890644550323486,
      "learning_rate": 8.576206402293359e-06,
      "loss": 1.8475,
      "step": 359
    },
    {
      "epoch": 1.0975609756097562,
      "grad_norm": 1.5532346963882446,
      "learning_rate": 8.600095556617297e-06,
      "loss": 1.8586,
      "step": 360
    },
    {
      "epoch": 1.100609756097561,
      "grad_norm": 1.334532618522644,
      "learning_rate": 8.623984710941233e-06,
      "loss": 1.8885,
      "step": 361
    },
    {
      "epoch": 1.103658536585366,
      "grad_norm": 1.677895188331604,
      "learning_rate": 8.64787386526517e-06,
      "loss": 1.8807,
      "step": 362
    },
    {
      "epoch": 1.1067073170731707,
      "grad_norm": 0.9365919232368469,
      "learning_rate": 8.671763019589108e-06,
      "loss": 1.8464,
      "step": 363
    },
    {
      "epoch": 1.1097560975609757,
      "grad_norm": 1.2400935888290405,
      "learning_rate": 8.695652173913044e-06,
      "loss": 1.8583,
      "step": 364
    },
    {
      "epoch": 1.1128048780487805,
      "grad_norm": 1.6579198837280273,
      "learning_rate": 8.71954132823698e-06,
      "loss": 1.9023,
      "step": 365
    },
    {
      "epoch": 1.1158536585365855,
      "grad_norm": 1.1340681314468384,
      "learning_rate": 8.743430482560916e-06,
      "loss": 1.8682,
      "step": 366
    },
    {
      "epoch": 1.1189024390243902,
      "grad_norm": 1.2545247077941895,
      "learning_rate": 8.767319636884854e-06,
      "loss": 1.8557,
      "step": 367
    },
    {
      "epoch": 1.1219512195121952,
      "grad_norm": 1.1948604583740234,
      "learning_rate": 8.791208791208792e-06,
      "loss": 1.841,
      "step": 368
    },
    {
      "epoch": 1.125,
      "grad_norm": 1.5377408266067505,
      "learning_rate": 8.815097945532728e-06,
      "loss": 1.869,
      "step": 369
    },
    {
      "epoch": 1.1280487804878048,
      "grad_norm": 1.1770614385604858,
      "learning_rate": 8.838987099856666e-06,
      "loss": 1.8573,
      "step": 370
    },
    {
      "epoch": 1.1310975609756098,
      "grad_norm": 1.547342300415039,
      "learning_rate": 8.862876254180602e-06,
      "loss": 1.87,
      "step": 371
    },
    {
      "epoch": 1.1341463414634148,
      "grad_norm": 1.2292367219924927,
      "learning_rate": 8.88676540850454e-06,
      "loss": 1.8619,
      "step": 372
    },
    {
      "epoch": 1.1371951219512195,
      "grad_norm": 1.4160205125808716,
      "learning_rate": 8.910654562828476e-06,
      "loss": 1.8345,
      "step": 373
    },
    {
      "epoch": 1.1402439024390243,
      "grad_norm": 1.2597169876098633,
      "learning_rate": 8.934543717152413e-06,
      "loss": 1.8566,
      "step": 374
    },
    {
      "epoch": 1.1432926829268293,
      "grad_norm": 1.399562120437622,
      "learning_rate": 8.958432871476351e-06,
      "loss": 1.8499,
      "step": 375
    },
    {
      "epoch": 1.146341463414634,
      "grad_norm": 1.7780927419662476,
      "learning_rate": 8.982322025800287e-06,
      "loss": 1.8786,
      "step": 376
    },
    {
      "epoch": 1.149390243902439,
      "grad_norm": 1.4580808877944946,
      "learning_rate": 9.006211180124225e-06,
      "loss": 1.8569,
      "step": 377
    },
    {
      "epoch": 1.1524390243902438,
      "grad_norm": 1.1808769702911377,
      "learning_rate": 9.03010033444816e-06,
      "loss": 1.8446,
      "step": 378
    },
    {
      "epoch": 1.1554878048780488,
      "grad_norm": 1.2066373825073242,
      "learning_rate": 9.053989488772099e-06,
      "loss": 1.8595,
      "step": 379
    },
    {
      "epoch": 1.1585365853658536,
      "grad_norm": 1.3586331605911255,
      "learning_rate": 9.077878643096036e-06,
      "loss": 1.8261,
      "step": 380
    },
    {
      "epoch": 1.1615853658536586,
      "grad_norm": 1.1528756618499756,
      "learning_rate": 9.101767797419972e-06,
      "loss": 1.8412,
      "step": 381
    },
    {
      "epoch": 1.1646341463414633,
      "grad_norm": 1.099196195602417,
      "learning_rate": 9.125656951743908e-06,
      "loss": 1.855,
      "step": 382
    },
    {
      "epoch": 1.1676829268292683,
      "grad_norm": 1.7174967527389526,
      "learning_rate": 9.149546106067846e-06,
      "loss": 1.8548,
      "step": 383
    },
    {
      "epoch": 1.170731707317073,
      "grad_norm": 1.3336364030838013,
      "learning_rate": 9.173435260391782e-06,
      "loss": 1.8193,
      "step": 384
    },
    {
      "epoch": 1.173780487804878,
      "grad_norm": 1.4370683431625366,
      "learning_rate": 9.197324414715718e-06,
      "loss": 1.8651,
      "step": 385
    },
    {
      "epoch": 1.1768292682926829,
      "grad_norm": 1.352620244026184,
      "learning_rate": 9.221213569039656e-06,
      "loss": 1.855,
      "step": 386
    },
    {
      "epoch": 1.1798780487804879,
      "grad_norm": 1.2003369331359863,
      "learning_rate": 9.245102723363594e-06,
      "loss": 1.8787,
      "step": 387
    },
    {
      "epoch": 1.1829268292682926,
      "grad_norm": 1.0103908777236938,
      "learning_rate": 9.26899187768753e-06,
      "loss": 1.861,
      "step": 388
    },
    {
      "epoch": 1.1859756097560976,
      "grad_norm": 1.1842619180679321,
      "learning_rate": 9.292881032011467e-06,
      "loss": 1.8564,
      "step": 389
    },
    {
      "epoch": 1.1890243902439024,
      "grad_norm": 1.016893744468689,
      "learning_rate": 9.316770186335403e-06,
      "loss": 1.8513,
      "step": 390
    },
    {
      "epoch": 1.1920731707317074,
      "grad_norm": 1.3235560655593872,
      "learning_rate": 9.340659340659341e-06,
      "loss": 1.8496,
      "step": 391
    },
    {
      "epoch": 1.1951219512195121,
      "grad_norm": 1.1269638538360596,
      "learning_rate": 9.364548494983277e-06,
      "loss": 1.8522,
      "step": 392
    },
    {
      "epoch": 1.1981707317073171,
      "grad_norm": 1.5220413208007812,
      "learning_rate": 9.388437649307215e-06,
      "loss": 1.8496,
      "step": 393
    },
    {
      "epoch": 1.201219512195122,
      "grad_norm": 1.3874478340148926,
      "learning_rate": 9.412326803631153e-06,
      "loss": 1.842,
      "step": 394
    },
    {
      "epoch": 1.204268292682927,
      "grad_norm": 1.173701286315918,
      "learning_rate": 9.436215957955089e-06,
      "loss": 1.8334,
      "step": 395
    },
    {
      "epoch": 1.2073170731707317,
      "grad_norm": 1.0839632749557495,
      "learning_rate": 9.460105112279026e-06,
      "loss": 1.8529,
      "step": 396
    },
    {
      "epoch": 1.2103658536585367,
      "grad_norm": 1.2691766023635864,
      "learning_rate": 9.483994266602962e-06,
      "loss": 1.8538,
      "step": 397
    },
    {
      "epoch": 1.2134146341463414,
      "grad_norm": 1.4501962661743164,
      "learning_rate": 9.5078834209269e-06,
      "loss": 1.8401,
      "step": 398
    },
    {
      "epoch": 1.2164634146341464,
      "grad_norm": 1.525980830192566,
      "learning_rate": 9.531772575250838e-06,
      "loss": 1.8384,
      "step": 399
    },
    {
      "epoch": 1.2195121951219512,
      "grad_norm": 1.5571147203445435,
      "learning_rate": 9.555661729574774e-06,
      "loss": 1.8425,
      "step": 400
    },
    {
      "epoch": 1.2225609756097562,
      "grad_norm": 1.214404582977295,
      "learning_rate": 9.57955088389871e-06,
      "loss": 1.8532,
      "step": 401
    },
    {
      "epoch": 1.225609756097561,
      "grad_norm": 1.6008305549621582,
      "learning_rate": 9.603440038222648e-06,
      "loss": 1.8634,
      "step": 402
    },
    {
      "epoch": 1.228658536585366,
      "grad_norm": 1.2446640729904175,
      "learning_rate": 9.627329192546584e-06,
      "loss": 1.8188,
      "step": 403
    },
    {
      "epoch": 1.2317073170731707,
      "grad_norm": 1.3831348419189453,
      "learning_rate": 9.65121834687052e-06,
      "loss": 1.86,
      "step": 404
    },
    {
      "epoch": 1.2347560975609757,
      "grad_norm": 1.5520838499069214,
      "learning_rate": 9.675107501194458e-06,
      "loss": 1.8354,
      "step": 405
    },
    {
      "epoch": 1.2378048780487805,
      "grad_norm": 1.7609459161758423,
      "learning_rate": 9.698996655518395e-06,
      "loss": 1.8638,
      "step": 406
    },
    {
      "epoch": 1.2408536585365852,
      "grad_norm": 1.3091081380844116,
      "learning_rate": 9.722885809842331e-06,
      "loss": 1.8547,
      "step": 407
    },
    {
      "epoch": 1.2439024390243902,
      "grad_norm": 1.2061724662780762,
      "learning_rate": 9.746774964166269e-06,
      "loss": 1.851,
      "step": 408
    },
    {
      "epoch": 1.2469512195121952,
      "grad_norm": 1.0506770610809326,
      "learning_rate": 9.770664118490205e-06,
      "loss": 1.8416,
      "step": 409
    },
    {
      "epoch": 1.25,
      "grad_norm": 1.1679924726486206,
      "learning_rate": 9.794553272814143e-06,
      "loss": 1.8268,
      "step": 410
    },
    {
      "epoch": 1.2530487804878048,
      "grad_norm": 1.7359397411346436,
      "learning_rate": 9.81844242713808e-06,
      "loss": 1.8286,
      "step": 411
    },
    {
      "epoch": 1.2560975609756098,
      "grad_norm": 1.7183328866958618,
      "learning_rate": 9.842331581462017e-06,
      "loss": 1.8144,
      "step": 412
    },
    {
      "epoch": 1.2591463414634148,
      "grad_norm": 1.5007789134979248,
      "learning_rate": 9.866220735785954e-06,
      "loss": 1.8388,
      "step": 413
    },
    {
      "epoch": 1.2621951219512195,
      "grad_norm": 1.10489821434021,
      "learning_rate": 9.89010989010989e-06,
      "loss": 1.8278,
      "step": 414
    },
    {
      "epoch": 1.2652439024390243,
      "grad_norm": 1.3604612350463867,
      "learning_rate": 9.913999044433828e-06,
      "loss": 1.869,
      "step": 415
    },
    {
      "epoch": 1.2682926829268293,
      "grad_norm": 1.2253962755203247,
      "learning_rate": 9.937888198757764e-06,
      "loss": 1.872,
      "step": 416
    },
    {
      "epoch": 1.2713414634146343,
      "grad_norm": 0.9022706151008606,
      "learning_rate": 9.961777353081702e-06,
      "loss": 1.8376,
      "step": 417
    },
    {
      "epoch": 1.274390243902439,
      "grad_norm": 1.6189152002334595,
      "learning_rate": 9.98566650740564e-06,
      "loss": 1.8282,
      "step": 418
    },
    {
      "epoch": 1.2774390243902438,
      "grad_norm": 1.2515215873718262,
      "learning_rate": 1.0009555661729576e-05,
      "loss": 1.8083,
      "step": 419
    },
    {
      "epoch": 1.2804878048780488,
      "grad_norm": 1.4213299751281738,
      "learning_rate": 1.0033444816053512e-05,
      "loss": 1.8359,
      "step": 420
    },
    {
      "epoch": 1.2835365853658536,
      "grad_norm": 1.1888097524642944,
      "learning_rate": 1.005733397037745e-05,
      "loss": 1.8279,
      "step": 421
    },
    {
      "epoch": 1.2865853658536586,
      "grad_norm": 1.259345293045044,
      "learning_rate": 1.0081223124701385e-05,
      "loss": 1.8297,
      "step": 422
    },
    {
      "epoch": 1.2896341463414633,
      "grad_norm": 1.1945879459381104,
      "learning_rate": 1.0105112279025323e-05,
      "loss": 1.8157,
      "step": 423
    },
    {
      "epoch": 1.2926829268292683,
      "grad_norm": 2.0065417289733887,
      "learning_rate": 1.0129001433349259e-05,
      "loss": 1.8262,
      "step": 424
    },
    {
      "epoch": 1.295731707317073,
      "grad_norm": 1.3807355165481567,
      "learning_rate": 1.0152890587673197e-05,
      "loss": 1.8493,
      "step": 425
    },
    {
      "epoch": 1.298780487804878,
      "grad_norm": 1.557835578918457,
      "learning_rate": 1.0176779741997133e-05,
      "loss": 1.8219,
      "step": 426
    },
    {
      "epoch": 1.3018292682926829,
      "grad_norm": 1.1452269554138184,
      "learning_rate": 1.020066889632107e-05,
      "loss": 1.8344,
      "step": 427
    },
    {
      "epoch": 1.3048780487804879,
      "grad_norm": 1.3135175704956055,
      "learning_rate": 1.0224558050645007e-05,
      "loss": 1.8327,
      "step": 428
    },
    {
      "epoch": 1.3079268292682926,
      "grad_norm": 1.7833805084228516,
      "learning_rate": 1.0248447204968944e-05,
      "loss": 1.8269,
      "step": 429
    },
    {
      "epoch": 1.3109756097560976,
      "grad_norm": 1.2576963901519775,
      "learning_rate": 1.0272336359292882e-05,
      "loss": 1.8141,
      "step": 430
    },
    {
      "epoch": 1.3140243902439024,
      "grad_norm": 1.8706259727478027,
      "learning_rate": 1.0296225513616818e-05,
      "loss": 1.8498,
      "step": 431
    },
    {
      "epoch": 1.3170731707317074,
      "grad_norm": 1.4509021043777466,
      "learning_rate": 1.0320114667940756e-05,
      "loss": 1.8196,
      "step": 432
    },
    {
      "epoch": 1.3201219512195121,
      "grad_norm": 1.1647748947143555,
      "learning_rate": 1.0344003822264692e-05,
      "loss": 1.8307,
      "step": 433
    },
    {
      "epoch": 1.3231707317073171,
      "grad_norm": 1.1964359283447266,
      "learning_rate": 1.036789297658863e-05,
      "loss": 1.833,
      "step": 434
    },
    {
      "epoch": 1.326219512195122,
      "grad_norm": 1.7886474132537842,
      "learning_rate": 1.0391782130912567e-05,
      "loss": 1.8446,
      "step": 435
    },
    {
      "epoch": 1.329268292682927,
      "grad_norm": 1.7904127836227417,
      "learning_rate": 1.0415671285236503e-05,
      "loss": 1.8302,
      "step": 436
    },
    {
      "epoch": 1.3323170731707317,
      "grad_norm": 1.2770018577575684,
      "learning_rate": 1.0439560439560441e-05,
      "loss": 1.8393,
      "step": 437
    },
    {
      "epoch": 1.3353658536585367,
      "grad_norm": 1.2134963274002075,
      "learning_rate": 1.0463449593884377e-05,
      "loss": 1.8319,
      "step": 438
    },
    {
      "epoch": 1.3384146341463414,
      "grad_norm": 1.6391620635986328,
      "learning_rate": 1.0487338748208313e-05,
      "loss": 1.861,
      "step": 439
    },
    {
      "epoch": 1.3414634146341464,
      "grad_norm": 1.6710244417190552,
      "learning_rate": 1.0511227902532251e-05,
      "loss": 1.8146,
      "step": 440
    },
    {
      "epoch": 1.3445121951219512,
      "grad_norm": 1.2160699367523193,
      "learning_rate": 1.0535117056856187e-05,
      "loss": 1.7981,
      "step": 441
    },
    {
      "epoch": 1.3475609756097562,
      "grad_norm": 1.4557509422302246,
      "learning_rate": 1.0559006211180125e-05,
      "loss": 1.8502,
      "step": 442
    },
    {
      "epoch": 1.350609756097561,
      "grad_norm": 1.143218994140625,
      "learning_rate": 1.058289536550406e-05,
      "loss": 1.7946,
      "step": 443
    },
    {
      "epoch": 1.3536585365853657,
      "grad_norm": 1.3378281593322754,
      "learning_rate": 1.0606784519827999e-05,
      "loss": 1.8026,
      "step": 444
    },
    {
      "epoch": 1.3567073170731707,
      "grad_norm": 1.0823650360107422,
      "learning_rate": 1.0630673674151935e-05,
      "loss": 1.8088,
      "step": 445
    },
    {
      "epoch": 1.3597560975609757,
      "grad_norm": 1.4258918762207031,
      "learning_rate": 1.0654562828475872e-05,
      "loss": 1.7823,
      "step": 446
    },
    {
      "epoch": 1.3628048780487805,
      "grad_norm": 1.4622936248779297,
      "learning_rate": 1.067845198279981e-05,
      "loss": 1.8225,
      "step": 447
    },
    {
      "epoch": 1.3658536585365852,
      "grad_norm": 1.2240326404571533,
      "learning_rate": 1.0702341137123746e-05,
      "loss": 1.8025,
      "step": 448
    },
    {
      "epoch": 1.3689024390243902,
      "grad_norm": 1.7447360754013062,
      "learning_rate": 1.0726230291447684e-05,
      "loss": 1.8204,
      "step": 449
    },
    {
      "epoch": 1.3719512195121952,
      "grad_norm": 0.8882580995559692,
      "learning_rate": 1.075011944577162e-05,
      "loss": 1.8311,
      "step": 450
    },
    {
      "epoch": 1.375,
      "grad_norm": 1.493490219116211,
      "learning_rate": 1.0774008600095558e-05,
      "loss": 1.796,
      "step": 451
    },
    {
      "epoch": 1.3780487804878048,
      "grad_norm": 1.5887157917022705,
      "learning_rate": 1.0797897754419494e-05,
      "loss": 1.8212,
      "step": 452
    },
    {
      "epoch": 1.3810975609756098,
      "grad_norm": 1.3426703214645386,
      "learning_rate": 1.0821786908743431e-05,
      "loss": 1.7932,
      "step": 453
    },
    {
      "epoch": 1.3841463414634148,
      "grad_norm": 1.3861593008041382,
      "learning_rate": 1.0845676063067369e-05,
      "loss": 1.819,
      "step": 454
    },
    {
      "epoch": 1.3871951219512195,
      "grad_norm": 1.1245893239974976,
      "learning_rate": 1.0869565217391305e-05,
      "loss": 1.7957,
      "step": 455
    },
    {
      "epoch": 1.3902439024390243,
      "grad_norm": 1.0648462772369385,
      "learning_rate": 1.0893454371715243e-05,
      "loss": 1.7664,
      "step": 456
    },
    {
      "epoch": 1.3932926829268293,
      "grad_norm": 1.4050796031951904,
      "learning_rate": 1.0917343526039179e-05,
      "loss": 1.7962,
      "step": 457
    },
    {
      "epoch": 1.3963414634146343,
      "grad_norm": 1.4061987400054932,
      "learning_rate": 1.0941232680363115e-05,
      "loss": 1.8066,
      "step": 458
    },
    {
      "epoch": 1.399390243902439,
      "grad_norm": 1.7711355686187744,
      "learning_rate": 1.0965121834687053e-05,
      "loss": 1.7996,
      "step": 459
    },
    {
      "epoch": 1.4024390243902438,
      "grad_norm": 1.317095160484314,
      "learning_rate": 1.0989010989010989e-05,
      "loss": 1.8203,
      "step": 460
    },
    {
      "epoch": 1.4054878048780488,
      "grad_norm": 1.2543864250183105,
      "learning_rate": 1.1012900143334926e-05,
      "loss": 1.7993,
      "step": 461
    },
    {
      "epoch": 1.4085365853658536,
      "grad_norm": 1.1948540210723877,
      "learning_rate": 1.1036789297658862e-05,
      "loss": 1.7985,
      "step": 462
    },
    {
      "epoch": 1.4115853658536586,
      "grad_norm": 1.1897531747817993,
      "learning_rate": 1.10606784519828e-05,
      "loss": 1.8249,
      "step": 463
    },
    {
      "epoch": 1.4146341463414633,
      "grad_norm": 1.3533622026443481,
      "learning_rate": 1.1084567606306736e-05,
      "loss": 1.7898,
      "step": 464
    },
    {
      "epoch": 1.4176829268292683,
      "grad_norm": 1.4774025678634644,
      "learning_rate": 1.1108456760630674e-05,
      "loss": 1.7913,
      "step": 465
    },
    {
      "epoch": 1.420731707317073,
      "grad_norm": 1.2749472856521606,
      "learning_rate": 1.1132345914954612e-05,
      "loss": 1.791,
      "step": 466
    },
    {
      "epoch": 1.423780487804878,
      "grad_norm": 1.590815544128418,
      "learning_rate": 1.1156235069278548e-05,
      "loss": 1.7796,
      "step": 467
    },
    {
      "epoch": 1.4268292682926829,
      "grad_norm": 1.6529972553253174,
      "learning_rate": 1.1180124223602485e-05,
      "loss": 1.8162,
      "step": 468
    },
    {
      "epoch": 1.4298780487804879,
      "grad_norm": 1.9321867227554321,
      "learning_rate": 1.1204013377926421e-05,
      "loss": 1.8285,
      "step": 469
    },
    {
      "epoch": 1.4329268292682926,
      "grad_norm": 1.2613098621368408,
      "learning_rate": 1.122790253225036e-05,
      "loss": 1.789,
      "step": 470
    },
    {
      "epoch": 1.4359756097560976,
      "grad_norm": 1.385195016860962,
      "learning_rate": 1.1251791686574297e-05,
      "loss": 1.792,
      "step": 471
    },
    {
      "epoch": 1.4390243902439024,
      "grad_norm": 1.5072375535964966,
      "learning_rate": 1.1275680840898233e-05,
      "loss": 1.8156,
      "step": 472
    },
    {
      "epoch": 1.4420731707317074,
      "grad_norm": 1.1720560789108276,
      "learning_rate": 1.129956999522217e-05,
      "loss": 1.8009,
      "step": 473
    },
    {
      "epoch": 1.4451219512195121,
      "grad_norm": 1.3699284791946411,
      "learning_rate": 1.1323459149546107e-05,
      "loss": 1.8285,
      "step": 474
    },
    {
      "epoch": 1.4481707317073171,
      "grad_norm": 1.699039101600647,
      "learning_rate": 1.1347348303870044e-05,
      "loss": 1.7736,
      "step": 475
    },
    {
      "epoch": 1.451219512195122,
      "grad_norm": 1.9093457460403442,
      "learning_rate": 1.137123745819398e-05,
      "loss": 1.8149,
      "step": 476
    },
    {
      "epoch": 1.454268292682927,
      "grad_norm": 1.0876573324203491,
      "learning_rate": 1.1395126612517917e-05,
      "loss": 1.7802,
      "step": 477
    },
    {
      "epoch": 1.4573170731707317,
      "grad_norm": 1.6443744897842407,
      "learning_rate": 1.1419015766841854e-05,
      "loss": 1.7556,
      "step": 478
    },
    {
      "epoch": 1.4603658536585367,
      "grad_norm": 1.3351290225982666,
      "learning_rate": 1.144290492116579e-05,
      "loss": 1.7947,
      "step": 479
    },
    {
      "epoch": 1.4634146341463414,
      "grad_norm": 1.2072430849075317,
      "learning_rate": 1.1466794075489728e-05,
      "loss": 1.8135,
      "step": 480
    },
    {
      "epoch": 1.4664634146341464,
      "grad_norm": 1.332467794418335,
      "learning_rate": 1.1490683229813664e-05,
      "loss": 1.7517,
      "step": 481
    },
    {
      "epoch": 1.4695121951219512,
      "grad_norm": 1.4976483583450317,
      "learning_rate": 1.1514572384137602e-05,
      "loss": 1.7667,
      "step": 482
    },
    {
      "epoch": 1.4725609756097562,
      "grad_norm": 1.4024944305419922,
      "learning_rate": 1.153846153846154e-05,
      "loss": 1.7784,
      "step": 483
    },
    {
      "epoch": 1.475609756097561,
      "grad_norm": 1.051856279373169,
      "learning_rate": 1.1562350692785476e-05,
      "loss": 1.7741,
      "step": 484
    },
    {
      "epoch": 1.4786585365853657,
      "grad_norm": 1.3871910572052002,
      "learning_rate": 1.1586239847109413e-05,
      "loss": 1.788,
      "step": 485
    },
    {
      "epoch": 1.4817073170731707,
      "grad_norm": 1.9038153886795044,
      "learning_rate": 1.161012900143335e-05,
      "loss": 1.8004,
      "step": 486
    },
    {
      "epoch": 1.4847560975609757,
      "grad_norm": 2.150568962097168,
      "learning_rate": 1.1634018155757287e-05,
      "loss": 1.8155,
      "step": 487
    },
    {
      "epoch": 1.4878048780487805,
      "grad_norm": 1.4318993091583252,
      "learning_rate": 1.1657907310081223e-05,
      "loss": 1.7799,
      "step": 488
    },
    {
      "epoch": 1.4908536585365852,
      "grad_norm": 1.1024727821350098,
      "learning_rate": 1.168179646440516e-05,
      "loss": 1.7385,
      "step": 489
    },
    {
      "epoch": 1.4939024390243902,
      "grad_norm": 1.1725809574127197,
      "learning_rate": 1.1705685618729099e-05,
      "loss": 1.7778,
      "step": 490
    },
    {
      "epoch": 1.4969512195121952,
      "grad_norm": 1.1580721139907837,
      "learning_rate": 1.1729574773053035e-05,
      "loss": 1.7676,
      "step": 491
    },
    {
      "epoch": 1.5,
      "grad_norm": 1.5615917444229126,
      "learning_rate": 1.1753463927376972e-05,
      "loss": 1.7604,
      "step": 492
    },
    {
      "epoch": 1.5030487804878048,
      "grad_norm": 1.3707205057144165,
      "learning_rate": 1.1777353081700908e-05,
      "loss": 1.7616,
      "step": 493
    },
    {
      "epoch": 1.5060975609756098,
      "grad_norm": 1.2371766567230225,
      "learning_rate": 1.1801242236024846e-05,
      "loss": 1.766,
      "step": 494
    },
    {
      "epoch": 1.5091463414634148,
      "grad_norm": 1.370705008506775,
      "learning_rate": 1.1825131390348782e-05,
      "loss": 1.7792,
      "step": 495
    },
    {
      "epoch": 1.5121951219512195,
      "grad_norm": 0.9594470858573914,
      "learning_rate": 1.1849020544672718e-05,
      "loss": 1.7796,
      "step": 496
    },
    {
      "epoch": 1.5152439024390243,
      "grad_norm": 0.9499321579933167,
      "learning_rate": 1.1872909698996656e-05,
      "loss": 1.7743,
      "step": 497
    },
    {
      "epoch": 1.5182926829268293,
      "grad_norm": 1.077327013015747,
      "learning_rate": 1.1896798853320592e-05,
      "loss": 1.7808,
      "step": 498
    },
    {
      "epoch": 1.5213414634146343,
      "grad_norm": 1.3082761764526367,
      "learning_rate": 1.192068800764453e-05,
      "loss": 1.7631,
      "step": 499
    },
    {
      "epoch": 1.524390243902439,
      "grad_norm": 1.567307710647583,
      "learning_rate": 1.1944577161968466e-05,
      "loss": 1.8037,
      "step": 500
    },
    {
      "epoch": 1.5274390243902438,
      "grad_norm": 1.7150704860687256,
      "learning_rate": 1.1968466316292403e-05,
      "loss": 1.7555,
      "step": 501
    },
    {
      "epoch": 1.5304878048780488,
      "grad_norm": 1.1174120903015137,
      "learning_rate": 1.1992355470616341e-05,
      "loss": 1.7716,
      "step": 502
    },
    {
      "epoch": 1.5335365853658538,
      "grad_norm": 0.9554036855697632,
      "learning_rate": 1.2016244624940277e-05,
      "loss": 1.7752,
      "step": 503
    },
    {
      "epoch": 1.5365853658536586,
      "grad_norm": 1.6881824731826782,
      "learning_rate": 1.2040133779264215e-05,
      "loss": 1.732,
      "step": 504
    },
    {
      "epoch": 1.5396341463414633,
      "grad_norm": 1.0422453880310059,
      "learning_rate": 1.2064022933588151e-05,
      "loss": 1.7409,
      "step": 505
    },
    {
      "epoch": 1.5426829268292683,
      "grad_norm": 1.5622003078460693,
      "learning_rate": 1.2087912087912089e-05,
      "loss": 1.8001,
      "step": 506
    },
    {
      "epoch": 1.5457317073170733,
      "grad_norm": 1.3496558666229248,
      "learning_rate": 1.2111801242236026e-05,
      "loss": 1.7906,
      "step": 507
    },
    {
      "epoch": 1.548780487804878,
      "grad_norm": 1.0862114429473877,
      "learning_rate": 1.2135690396559962e-05,
      "loss": 1.7669,
      "step": 508
    },
    {
      "epoch": 1.5518292682926829,
      "grad_norm": 1.2729769945144653,
      "learning_rate": 1.21595795508839e-05,
      "loss": 1.7504,
      "step": 509
    },
    {
      "epoch": 1.5548780487804879,
      "grad_norm": 1.5610710382461548,
      "learning_rate": 1.2183468705207836e-05,
      "loss": 1.8031,
      "step": 510
    },
    {
      "epoch": 1.5579268292682928,
      "grad_norm": 1.3541982173919678,
      "learning_rate": 1.2207357859531774e-05,
      "loss": 1.7387,
      "step": 511
    },
    {
      "epoch": 1.5609756097560976,
      "grad_norm": 1.4909765720367432,
      "learning_rate": 1.223124701385571e-05,
      "loss": 1.7521,
      "step": 512
    },
    {
      "epoch": 1.5640243902439024,
      "grad_norm": 1.249140977859497,
      "learning_rate": 1.2255136168179648e-05,
      "loss": 1.7552,
      "step": 513
    },
    {
      "epoch": 1.5670731707317072,
      "grad_norm": 1.6595475673675537,
      "learning_rate": 1.2279025322503584e-05,
      "loss": 1.7285,
      "step": 514
    },
    {
      "epoch": 1.5701219512195121,
      "grad_norm": 1.4804610013961792,
      "learning_rate": 1.230291447682752e-05,
      "loss": 1.7351,
      "step": 515
    },
    {
      "epoch": 1.5731707317073171,
      "grad_norm": 1.1619696617126465,
      "learning_rate": 1.2326803631151458e-05,
      "loss": 1.7651,
      "step": 516
    },
    {
      "epoch": 1.576219512195122,
      "grad_norm": 1.3458689451217651,
      "learning_rate": 1.2350692785475394e-05,
      "loss": 1.7627,
      "step": 517
    },
    {
      "epoch": 1.5792682926829267,
      "grad_norm": 1.1219476461410522,
      "learning_rate": 1.2374581939799331e-05,
      "loss": 1.7829,
      "step": 518
    },
    {
      "epoch": 1.5823170731707317,
      "grad_norm": 1.3169126510620117,
      "learning_rate": 1.2398471094123269e-05,
      "loss": 1.7139,
      "step": 519
    },
    {
      "epoch": 1.5853658536585367,
      "grad_norm": 1.5801228284835815,
      "learning_rate": 1.2422360248447205e-05,
      "loss": 1.7646,
      "step": 520
    },
    {
      "epoch": 1.5884146341463414,
      "grad_norm": 1.2792104482650757,
      "learning_rate": 1.2446249402771143e-05,
      "loss": 1.7475,
      "step": 521
    },
    {
      "epoch": 1.5914634146341462,
      "grad_norm": 1.6096501350402832,
      "learning_rate": 1.2470138557095079e-05,
      "loss": 1.767,
      "step": 522
    },
    {
      "epoch": 1.5945121951219512,
      "grad_norm": 1.244556188583374,
      "learning_rate": 1.2494027711419017e-05,
      "loss": 1.7493,
      "step": 523
    },
    {
      "epoch": 1.5975609756097562,
      "grad_norm": 1.1291520595550537,
      "learning_rate": 1.2517916865742954e-05,
      "loss": 1.7673,
      "step": 524
    },
    {
      "epoch": 1.600609756097561,
      "grad_norm": 1.7580763101577759,
      "learning_rate": 1.254180602006689e-05,
      "loss": 1.7345,
      "step": 525
    },
    {
      "epoch": 1.6036585365853657,
      "grad_norm": 1.1370688676834106,
      "learning_rate": 1.2565695174390826e-05,
      "loss": 1.7551,
      "step": 526
    },
    {
      "epoch": 1.6067073170731707,
      "grad_norm": 1.8500752449035645,
      "learning_rate": 1.2589584328714766e-05,
      "loss": 1.7773,
      "step": 527
    },
    {
      "epoch": 1.6097560975609757,
      "grad_norm": 1.2471511363983154,
      "learning_rate": 1.2613473483038702e-05,
      "loss": 1.7192,
      "step": 528
    },
    {
      "epoch": 1.6128048780487805,
      "grad_norm": 1.397606611251831,
      "learning_rate": 1.2637362637362638e-05,
      "loss": 1.7312,
      "step": 529
    },
    {
      "epoch": 1.6158536585365852,
      "grad_norm": 1.149452805519104,
      "learning_rate": 1.2661251791686574e-05,
      "loss": 1.7331,
      "step": 530
    },
    {
      "epoch": 1.6189024390243902,
      "grad_norm": 1.7610794305801392,
      "learning_rate": 1.2685140946010512e-05,
      "loss": 1.735,
      "step": 531
    },
    {
      "epoch": 1.6219512195121952,
      "grad_norm": 1.871883749961853,
      "learning_rate": 1.270903010033445e-05,
      "loss": 1.6859,
      "step": 532
    },
    {
      "epoch": 1.625,
      "grad_norm": 1.1732652187347412,
      "learning_rate": 1.2732919254658385e-05,
      "loss": 1.7179,
      "step": 533
    },
    {
      "epoch": 1.6280487804878048,
      "grad_norm": 1.335120677947998,
      "learning_rate": 1.2756808408982323e-05,
      "loss": 1.6995,
      "step": 534
    },
    {
      "epoch": 1.6310975609756098,
      "grad_norm": 1.2054011821746826,
      "learning_rate": 1.278069756330626e-05,
      "loss": 1.6944,
      "step": 535
    },
    {
      "epoch": 1.6341463414634148,
      "grad_norm": 1.8137258291244507,
      "learning_rate": 1.2804586717630195e-05,
      "loss": 1.7218,
      "step": 536
    },
    {
      "epoch": 1.6371951219512195,
      "grad_norm": 0.9834017753601074,
      "learning_rate": 1.2828475871954135e-05,
      "loss": 1.7236,
      "step": 537
    },
    {
      "epoch": 1.6402439024390243,
      "grad_norm": 1.0675808191299438,
      "learning_rate": 1.285236502627807e-05,
      "loss": 1.7263,
      "step": 538
    },
    {
      "epoch": 1.6432926829268293,
      "grad_norm": 1.427412748336792,
      "learning_rate": 1.2876254180602007e-05,
      "loss": 1.704,
      "step": 539
    },
    {
      "epoch": 1.6463414634146343,
      "grad_norm": 1.826236367225647,
      "learning_rate": 1.2900143334925943e-05,
      "loss": 1.7056,
      "step": 540
    },
    {
      "epoch": 1.649390243902439,
      "grad_norm": 1.675380825996399,
      "learning_rate": 1.2924032489249882e-05,
      "loss": 1.7243,
      "step": 541
    },
    {
      "epoch": 1.6524390243902438,
      "grad_norm": 1.547176718711853,
      "learning_rate": 1.2947921643573818e-05,
      "loss": 1.7588,
      "step": 542
    },
    {
      "epoch": 1.6554878048780488,
      "grad_norm": 1.2663620710372925,
      "learning_rate": 1.2971810797897754e-05,
      "loss": 1.7283,
      "step": 543
    },
    {
      "epoch": 1.6585365853658538,
      "grad_norm": 1.0774614810943604,
      "learning_rate": 1.2995699952221694e-05,
      "loss": 1.7289,
      "step": 544
    },
    {
      "epoch": 1.6615853658536586,
      "grad_norm": 1.224526286125183,
      "learning_rate": 1.301958910654563e-05,
      "loss": 1.7314,
      "step": 545
    },
    {
      "epoch": 1.6646341463414633,
      "grad_norm": 1.4826956987380981,
      "learning_rate": 1.3043478260869566e-05,
      "loss": 1.6792,
      "step": 546
    },
    {
      "epoch": 1.6676829268292683,
      "grad_norm": 1.5240411758422852,
      "learning_rate": 1.3067367415193502e-05,
      "loss": 1.7494,
      "step": 547
    },
    {
      "epoch": 1.6707317073170733,
      "grad_norm": 1.5709624290466309,
      "learning_rate": 1.3091256569517441e-05,
      "loss": 1.701,
      "step": 548
    },
    {
      "epoch": 1.673780487804878,
      "grad_norm": 1.178803563117981,
      "learning_rate": 1.3115145723841377e-05,
      "loss": 1.7593,
      "step": 549
    },
    {
      "epoch": 1.6768292682926829,
      "grad_norm": 1.28700590133667,
      "learning_rate": 1.3139034878165313e-05,
      "loss": 1.7332,
      "step": 550
    },
    {
      "epoch": 1.6798780487804879,
      "grad_norm": 1.50022554397583,
      "learning_rate": 1.3162924032489251e-05,
      "loss": 1.7516,
      "step": 551
    },
    {
      "epoch": 1.6829268292682928,
      "grad_norm": 1.6326510906219482,
      "learning_rate": 1.3186813186813187e-05,
      "loss": 1.7078,
      "step": 552
    },
    {
      "epoch": 1.6859756097560976,
      "grad_norm": 1.0618194341659546,
      "learning_rate": 1.3210702341137123e-05,
      "loss": 1.7522,
      "step": 553
    },
    {
      "epoch": 1.6890243902439024,
      "grad_norm": 1.9102654457092285,
      "learning_rate": 1.323459149546106e-05,
      "loss": 1.7545,
      "step": 554
    },
    {
      "epoch": 1.6920731707317072,
      "grad_norm": 1.560104250907898,
      "learning_rate": 1.3258480649784999e-05,
      "loss": 1.6963,
      "step": 555
    },
    {
      "epoch": 1.6951219512195121,
      "grad_norm": 1.4017246961593628,
      "learning_rate": 1.3282369804108935e-05,
      "loss": 1.7109,
      "step": 556
    },
    {
      "epoch": 1.6981707317073171,
      "grad_norm": 1.5348743200302124,
      "learning_rate": 1.330625895843287e-05,
      "loss": 1.7558,
      "step": 557
    },
    {
      "epoch": 1.701219512195122,
      "grad_norm": 1.3078914880752563,
      "learning_rate": 1.333014811275681e-05,
      "loss": 1.7331,
      "step": 558
    },
    {
      "epoch": 1.7042682926829267,
      "grad_norm": 1.3055058717727661,
      "learning_rate": 1.3354037267080746e-05,
      "loss": 1.7359,
      "step": 559
    },
    {
      "epoch": 1.7073170731707317,
      "grad_norm": 1.0244487524032593,
      "learning_rate": 1.3377926421404682e-05,
      "loss": 1.7074,
      "step": 560
    },
    {
      "epoch": 1.7103658536585367,
      "grad_norm": 0.9230087399482727,
      "learning_rate": 1.3401815575728622e-05,
      "loss": 1.7268,
      "step": 561
    },
    {
      "epoch": 1.7134146341463414,
      "grad_norm": 1.6395163536071777,
      "learning_rate": 1.3425704730052558e-05,
      "loss": 1.7517,
      "step": 562
    },
    {
      "epoch": 1.7164634146341462,
      "grad_norm": 1.4778075218200684,
      "learning_rate": 1.3449593884376494e-05,
      "loss": 1.7576,
      "step": 563
    },
    {
      "epoch": 1.7195121951219512,
      "grad_norm": 1.3275227546691895,
      "learning_rate": 1.347348303870043e-05,
      "loss": 1.6883,
      "step": 564
    },
    {
      "epoch": 1.7225609756097562,
      "grad_norm": 1.7379093170166016,
      "learning_rate": 1.3497372193024369e-05,
      "loss": 1.6767,
      "step": 565
    },
    {
      "epoch": 1.725609756097561,
      "grad_norm": 1.457944631576538,
      "learning_rate": 1.3521261347348305e-05,
      "loss": 1.7279,
      "step": 566
    },
    {
      "epoch": 1.7286585365853657,
      "grad_norm": 1.3680161237716675,
      "learning_rate": 1.3545150501672241e-05,
      "loss": 1.6888,
      "step": 567
    },
    {
      "epoch": 1.7317073170731707,
      "grad_norm": 1.0882847309112549,
      "learning_rate": 1.3569039655996179e-05,
      "loss": 1.7105,
      "step": 568
    },
    {
      "epoch": 1.7347560975609757,
      "grad_norm": 1.2895103693008423,
      "learning_rate": 1.3592928810320115e-05,
      "loss": 1.7238,
      "step": 569
    },
    {
      "epoch": 1.7378048780487805,
      "grad_norm": 1.92684006690979,
      "learning_rate": 1.3616817964644053e-05,
      "loss": 1.8123,
      "step": 570
    },
    {
      "epoch": 1.7408536585365852,
      "grad_norm": 1.2549277544021606,
      "learning_rate": 1.3640707118967989e-05,
      "loss": 1.721,
      "step": 571
    },
    {
      "epoch": 1.7439024390243902,
      "grad_norm": 1.9735279083251953,
      "learning_rate": 1.3664596273291926e-05,
      "loss": 1.7513,
      "step": 572
    },
    {
      "epoch": 1.7469512195121952,
      "grad_norm": 1.2220196723937988,
      "learning_rate": 1.3688485427615862e-05,
      "loss": 1.7322,
      "step": 573
    },
    {
      "epoch": 1.75,
      "grad_norm": 1.6588759422302246,
      "learning_rate": 1.3712374581939799e-05,
      "loss": 1.6511,
      "step": 574
    },
    {
      "epoch": 1.7530487804878048,
      "grad_norm": 1.5421390533447266,
      "learning_rate": 1.3736263736263738e-05,
      "loss": 1.7072,
      "step": 575
    },
    {
      "epoch": 1.7560975609756098,
      "grad_norm": 1.0815852880477905,
      "learning_rate": 1.3760152890587674e-05,
      "loss": 1.7167,
      "step": 576
    },
    {
      "epoch": 1.7591463414634148,
      "grad_norm": 1.3358503580093384,
      "learning_rate": 1.378404204491161e-05,
      "loss": 1.6708,
      "step": 577
    },
    {
      "epoch": 1.7621951219512195,
      "grad_norm": 1.784127950668335,
      "learning_rate": 1.3807931199235546e-05,
      "loss": 1.7304,
      "step": 578
    },
    {
      "epoch": 1.7652439024390243,
      "grad_norm": 1.2430269718170166,
      "learning_rate": 1.3831820353559485e-05,
      "loss": 1.6994,
      "step": 579
    },
    {
      "epoch": 1.7682926829268293,
      "grad_norm": 1.140137791633606,
      "learning_rate": 1.3855709507883422e-05,
      "loss": 1.6876,
      "step": 580
    },
    {
      "epoch": 1.7713414634146343,
      "grad_norm": 1.325613260269165,
      "learning_rate": 1.3879598662207358e-05,
      "loss": 1.6994,
      "step": 581
    },
    {
      "epoch": 1.774390243902439,
      "grad_norm": 1.6465643644332886,
      "learning_rate": 1.3903487816531297e-05,
      "loss": 1.7238,
      "step": 582
    },
    {
      "epoch": 1.7774390243902438,
      "grad_norm": 1.370643973350525,
      "learning_rate": 1.3927376970855233e-05,
      "loss": 1.6847,
      "step": 583
    },
    {
      "epoch": 1.7804878048780488,
      "grad_norm": 1.503752589225769,
      "learning_rate": 1.3951266125179169e-05,
      "loss": 1.729,
      "step": 584
    },
    {
      "epoch": 1.7835365853658538,
      "grad_norm": 1.9911935329437256,
      "learning_rate": 1.3975155279503105e-05,
      "loss": 1.6239,
      "step": 585
    },
    {
      "epoch": 1.7865853658536586,
      "grad_norm": 1.4479256868362427,
      "learning_rate": 1.3999044433827045e-05,
      "loss": 1.6693,
      "step": 586
    },
    {
      "epoch": 1.7896341463414633,
      "grad_norm": 1.4187835454940796,
      "learning_rate": 1.402293358815098e-05,
      "loss": 1.7005,
      "step": 587
    },
    {
      "epoch": 1.7926829268292683,
      "grad_norm": 1.5559972524642944,
      "learning_rate": 1.4046822742474917e-05,
      "loss": 1.7109,
      "step": 588
    },
    {
      "epoch": 1.7957317073170733,
      "grad_norm": 1.192073941230774,
      "learning_rate": 1.4070711896798854e-05,
      "loss": 1.7216,
      "step": 589
    },
    {
      "epoch": 1.798780487804878,
      "grad_norm": 1.502693772315979,
      "learning_rate": 1.409460105112279e-05,
      "loss": 1.7091,
      "step": 590
    },
    {
      "epoch": 1.8018292682926829,
      "grad_norm": 1.547672152519226,
      "learning_rate": 1.4118490205446726e-05,
      "loss": 1.6569,
      "step": 591
    },
    {
      "epoch": 1.8048780487804879,
      "grad_norm": 1.1669979095458984,
      "learning_rate": 1.4142379359770666e-05,
      "loss": 1.6588,
      "step": 592
    },
    {
      "epoch": 1.8079268292682928,
      "grad_norm": 1.3359887599945068,
      "learning_rate": 1.4166268514094602e-05,
      "loss": 1.6883,
      "step": 593
    },
    {
      "epoch": 1.8109756097560976,
      "grad_norm": 1.4787193536758423,
      "learning_rate": 1.4190157668418538e-05,
      "loss": 1.7003,
      "step": 594
    },
    {
      "epoch": 1.8140243902439024,
      "grad_norm": 1.0960140228271484,
      "learning_rate": 1.4214046822742474e-05,
      "loss": 1.6934,
      "step": 595
    },
    {
      "epoch": 1.8170731707317072,
      "grad_norm": 1.8068575859069824,
      "learning_rate": 1.4237935977066413e-05,
      "loss": 1.6842,
      "step": 596
    },
    {
      "epoch": 1.8201219512195121,
      "grad_norm": 1.0907670259475708,
      "learning_rate": 1.426182513139035e-05,
      "loss": 1.7043,
      "step": 597
    },
    {
      "epoch": 1.8231707317073171,
      "grad_norm": 1.3894389867782593,
      "learning_rate": 1.4285714285714285e-05,
      "loss": 1.6711,
      "step": 598
    },
    {
      "epoch": 1.826219512195122,
      "grad_norm": 1.0778388977050781,
      "learning_rate": 1.4309603440038225e-05,
      "loss": 1.7506,
      "step": 599
    },
    {
      "epoch": 1.8292682926829267,
      "grad_norm": 1.2185221910476685,
      "learning_rate": 1.4333492594362161e-05,
      "loss": 1.687,
      "step": 600
    },
    {
      "epoch": 1.8323170731707317,
      "grad_norm": 2.3873889446258545,
      "learning_rate": 1.4357381748686097e-05,
      "loss": 1.6861,
      "step": 601
    },
    {
      "epoch": 1.8353658536585367,
      "grad_norm": 1.7806200981140137,
      "learning_rate": 1.4381270903010033e-05,
      "loss": 1.6317,
      "step": 602
    },
    {
      "epoch": 1.8384146341463414,
      "grad_norm": 1.3936498165130615,
      "learning_rate": 1.4405160057333972e-05,
      "loss": 1.7021,
      "step": 603
    },
    {
      "epoch": 1.8414634146341462,
      "grad_norm": 1.5446096658706665,
      "learning_rate": 1.4429049211657908e-05,
      "loss": 1.7082,
      "step": 604
    },
    {
      "epoch": 1.8445121951219512,
      "grad_norm": 1.4810495376586914,
      "learning_rate": 1.4452938365981844e-05,
      "loss": 1.6586,
      "step": 605
    },
    {
      "epoch": 1.8475609756097562,
      "grad_norm": 1.741837978363037,
      "learning_rate": 1.4476827520305782e-05,
      "loss": 1.6981,
      "step": 606
    },
    {
      "epoch": 1.850609756097561,
      "grad_norm": 1.745367169380188,
      "learning_rate": 1.4500716674629718e-05,
      "loss": 1.7222,
      "step": 607
    },
    {
      "epoch": 1.8536585365853657,
      "grad_norm": 1.3141193389892578,
      "learning_rate": 1.4524605828953656e-05,
      "loss": 1.6896,
      "step": 608
    },
    {
      "epoch": 1.8567073170731707,
      "grad_norm": 1.4975403547286987,
      "learning_rate": 1.4548494983277592e-05,
      "loss": 1.6679,
      "step": 609
    },
    {
      "epoch": 1.8597560975609757,
      "grad_norm": 1.4005905389785767,
      "learning_rate": 1.457238413760153e-05,
      "loss": 1.6617,
      "step": 610
    },
    {
      "epoch": 1.8628048780487805,
      "grad_norm": 1.2360687255859375,
      "learning_rate": 1.4596273291925466e-05,
      "loss": 1.6877,
      "step": 611
    },
    {
      "epoch": 1.8658536585365852,
      "grad_norm": 1.2345179319381714,
      "learning_rate": 1.4620162446249402e-05,
      "loss": 1.6593,
      "step": 612
    },
    {
      "epoch": 1.8689024390243902,
      "grad_norm": 1.1534159183502197,
      "learning_rate": 1.4644051600573341e-05,
      "loss": 1.6865,
      "step": 613
    },
    {
      "epoch": 1.8719512195121952,
      "grad_norm": 1.528233289718628,
      "learning_rate": 1.4667940754897277e-05,
      "loss": 1.6185,
      "step": 614
    },
    {
      "epoch": 1.875,
      "grad_norm": 1.3682242631912231,
      "learning_rate": 1.4691829909221213e-05,
      "loss": 1.6519,
      "step": 615
    },
    {
      "epoch": 1.8780487804878048,
      "grad_norm": 1.3033708333969116,
      "learning_rate": 1.4715719063545153e-05,
      "loss": 1.6183,
      "step": 616
    },
    {
      "epoch": 1.8810975609756098,
      "grad_norm": 1.6784083843231201,
      "learning_rate": 1.4739608217869089e-05,
      "loss": 1.6795,
      "step": 617
    },
    {
      "epoch": 1.8841463414634148,
      "grad_norm": 1.6268645524978638,
      "learning_rate": 1.4763497372193025e-05,
      "loss": 1.6506,
      "step": 618
    },
    {
      "epoch": 1.8871951219512195,
      "grad_norm": 1.570094347000122,
      "learning_rate": 1.478738652651696e-05,
      "loss": 1.6339,
      "step": 619
    },
    {
      "epoch": 1.8902439024390243,
      "grad_norm": 1.2819561958312988,
      "learning_rate": 1.48112756808409e-05,
      "loss": 1.6457,
      "step": 620
    },
    {
      "epoch": 1.8932926829268293,
      "grad_norm": 1.0435129404067993,
      "learning_rate": 1.4835164835164836e-05,
      "loss": 1.6766,
      "step": 621
    },
    {
      "epoch": 1.8963414634146343,
      "grad_norm": 0.8963245153427124,
      "learning_rate": 1.4859053989488772e-05,
      "loss": 1.6351,
      "step": 622
    },
    {
      "epoch": 1.899390243902439,
      "grad_norm": 1.2988046407699585,
      "learning_rate": 1.4882943143812712e-05,
      "loss": 1.7008,
      "step": 623
    },
    {
      "epoch": 1.9024390243902438,
      "grad_norm": 1.2776038646697998,
      "learning_rate": 1.4906832298136648e-05,
      "loss": 1.6276,
      "step": 624
    },
    {
      "epoch": 1.9054878048780488,
      "grad_norm": 1.4377071857452393,
      "learning_rate": 1.4930721452460584e-05,
      "loss": 1.6461,
      "step": 625
    },
    {
      "epoch": 1.9085365853658538,
      "grad_norm": 1.7092266082763672,
      "learning_rate": 1.495461060678452e-05,
      "loss": 1.6984,
      "step": 626
    },
    {
      "epoch": 1.9115853658536586,
      "grad_norm": 1.312211513519287,
      "learning_rate": 1.4978499761108458e-05,
      "loss": 1.6744,
      "step": 627
    },
    {
      "epoch": 1.9146341463414633,
      "grad_norm": 1.191175103187561,
      "learning_rate": 1.5002388915432394e-05,
      "loss": 1.6551,
      "step": 628
    },
    {
      "epoch": 1.9176829268292683,
      "grad_norm": 1.2100468873977661,
      "learning_rate": 1.502627806975633e-05,
      "loss": 1.6769,
      "step": 629
    },
    {
      "epoch": 1.9207317073170733,
      "grad_norm": 1.3751288652420044,
      "learning_rate": 1.5050167224080269e-05,
      "loss": 1.6201,
      "step": 630
    },
    {
      "epoch": 1.923780487804878,
      "grad_norm": 1.4697462320327759,
      "learning_rate": 1.5074056378404205e-05,
      "loss": 1.6184,
      "step": 631
    },
    {
      "epoch": 1.9268292682926829,
      "grad_norm": 1.717488408088684,
      "learning_rate": 1.5097945532728141e-05,
      "loss": 1.6585,
      "step": 632
    },
    {
      "epoch": 1.9298780487804879,
      "grad_norm": 1.6435880661010742,
      "learning_rate": 1.5121834687052077e-05,
      "loss": 1.6579,
      "step": 633
    },
    {
      "epoch": 1.9329268292682928,
      "grad_norm": 1.32638680934906,
      "learning_rate": 1.5145723841376017e-05,
      "loss": 1.6103,
      "step": 634
    },
    {
      "epoch": 1.9359756097560976,
      "grad_norm": 1.3278546333312988,
      "learning_rate": 1.5169612995699953e-05,
      "loss": 1.6443,
      "step": 635
    },
    {
      "epoch": 1.9390243902439024,
      "grad_norm": 1.4124394655227661,
      "learning_rate": 1.5193502150023889e-05,
      "loss": 1.5802,
      "step": 636
    },
    {
      "epoch": 1.9420731707317072,
      "grad_norm": 1.2877939939498901,
      "learning_rate": 1.5217391304347828e-05,
      "loss": 1.6146,
      "step": 637
    },
    {
      "epoch": 1.9451219512195121,
      "grad_norm": 1.5262621641159058,
      "learning_rate": 1.5241280458671764e-05,
      "loss": 1.606,
      "step": 638
    },
    {
      "epoch": 1.9481707317073171,
      "grad_norm": 1.463170051574707,
      "learning_rate": 1.52651696129957e-05,
      "loss": 1.6338,
      "step": 639
    },
    {
      "epoch": 1.951219512195122,
      "grad_norm": 1.415116310119629,
      "learning_rate": 1.528905876731964e-05,
      "loss": 1.5866,
      "step": 640
    },
    {
      "epoch": 1.9542682926829267,
      "grad_norm": 1.1375330686569214,
      "learning_rate": 1.5312947921643576e-05,
      "loss": 1.6487,
      "step": 641
    },
    {
      "epoch": 1.9573170731707317,
      "grad_norm": 1.2722845077514648,
      "learning_rate": 1.5336837075967512e-05,
      "loss": 1.6526,
      "step": 642
    },
    {
      "epoch": 1.9603658536585367,
      "grad_norm": 1.8178693056106567,
      "learning_rate": 1.5360726230291448e-05,
      "loss": 1.6372,
      "step": 643
    },
    {
      "epoch": 1.9634146341463414,
      "grad_norm": 1.1473054885864258,
      "learning_rate": 1.5384615384615387e-05,
      "loss": 1.5848,
      "step": 644
    },
    {
      "epoch": 1.9664634146341462,
      "grad_norm": 1.5681153535842896,
      "learning_rate": 1.5408504538939323e-05,
      "loss": 1.6725,
      "step": 645
    },
    {
      "epoch": 1.9695121951219512,
      "grad_norm": 1.248457908630371,
      "learning_rate": 1.543239369326326e-05,
      "loss": 1.5939,
      "step": 646
    },
    {
      "epoch": 1.9725609756097562,
      "grad_norm": 1.1648986339569092,
      "learning_rate": 1.54562828475872e-05,
      "loss": 1.6465,
      "step": 647
    },
    {
      "epoch": 1.975609756097561,
      "grad_norm": 1.2327258586883545,
      "learning_rate": 1.5480172001911135e-05,
      "loss": 1.5941,
      "step": 648
    },
    {
      "epoch": 1.9786585365853657,
      "grad_norm": 1.5055632591247559,
      "learning_rate": 1.550406115623507e-05,
      "loss": 1.6318,
      "step": 649
    },
    {
      "epoch": 1.9817073170731707,
      "grad_norm": 1.2300368547439575,
      "learning_rate": 1.5527950310559007e-05,
      "loss": 1.5749,
      "step": 650
    },
    {
      "epoch": 1.9847560975609757,
      "grad_norm": 1.3913068771362305,
      "learning_rate": 1.5551839464882946e-05,
      "loss": 1.6093,
      "step": 651
    },
    {
      "epoch": 1.9878048780487805,
      "grad_norm": 1.4209049940109253,
      "learning_rate": 1.5575728619206882e-05,
      "loss": 1.5591,
      "step": 652
    },
    {
      "epoch": 1.9908536585365852,
      "grad_norm": 1.1387933492660522,
      "learning_rate": 1.5599617773530818e-05,
      "loss": 1.6726,
      "step": 653
    },
    {
      "epoch": 1.9939024390243902,
      "grad_norm": 1.2892252206802368,
      "learning_rate": 1.5623506927854754e-05,
      "loss": 1.5889,
      "step": 654
    },
    {
      "epoch": 1.9969512195121952,
      "grad_norm": 1.1516399383544922,
      "learning_rate": 1.564739608217869e-05,
      "loss": 1.5646,
      "step": 655
    },
    {
      "epoch": 2.0,
      "grad_norm": 5.504045009613037,
      "learning_rate": 1.5671285236502626e-05,
      "loss": 1.6936,
      "step": 656
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.8528849828047382,
      "eval_loss": 1.584693193435669,
      "eval_model_preparation_time": 0.0015,
      "eval_runtime": 3.1931,
      "eval_samples_per_second": 819.583,
      "eval_steps_per_second": 25.68,
      "step": 656
    }
  ],
  "logging_steps": 1,
  "max_steps": 656,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 116918078231880.0,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
