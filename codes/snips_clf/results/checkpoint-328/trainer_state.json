{
  "best_metric": 1.8722717761993408,
  "best_model_checkpoint": "./snips_clf/results/checkpoint-328",
  "epoch": 1.0,
  "eval_steps": 50,
  "global_step": 328,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.003048780487804878,
      "grad_norm": 0.9622209668159485,
      "learning_rate": 2.3889154323936934e-08,
      "loss": 1.9193,
      "step": 1
    },
    {
      "epoch": 0.006097560975609756,
      "grad_norm": 1.1136653423309326,
      "learning_rate": 4.777830864787387e-08,
      "loss": 1.9456,
      "step": 2
    },
    {
      "epoch": 0.009146341463414634,
      "grad_norm": 2.012254238128662,
      "learning_rate": 7.16674629718108e-08,
      "loss": 1.9882,
      "step": 3
    },
    {
      "epoch": 0.012195121951219513,
      "grad_norm": 1.35887610912323,
      "learning_rate": 9.555661729574773e-08,
      "loss": 1.9208,
      "step": 4
    },
    {
      "epoch": 0.01524390243902439,
      "grad_norm": 1.7767150402069092,
      "learning_rate": 1.1944577161968468e-07,
      "loss": 1.9301,
      "step": 5
    },
    {
      "epoch": 0.018292682926829267,
      "grad_norm": 1.479033350944519,
      "learning_rate": 1.433349259436216e-07,
      "loss": 1.9623,
      "step": 6
    },
    {
      "epoch": 0.021341463414634148,
      "grad_norm": 1.357845425605774,
      "learning_rate": 1.6722408026755853e-07,
      "loss": 1.9643,
      "step": 7
    },
    {
      "epoch": 0.024390243902439025,
      "grad_norm": 0.8007515072822571,
      "learning_rate": 1.9111323459149547e-07,
      "loss": 1.9415,
      "step": 8
    },
    {
      "epoch": 0.027439024390243903,
      "grad_norm": 1.198912501335144,
      "learning_rate": 2.150023889154324e-07,
      "loss": 1.9535,
      "step": 9
    },
    {
      "epoch": 0.03048780487804878,
      "grad_norm": 1.7378391027450562,
      "learning_rate": 2.3889154323936937e-07,
      "loss": 1.9889,
      "step": 10
    },
    {
      "epoch": 0.03353658536585366,
      "grad_norm": 1.528782606124878,
      "learning_rate": 2.6278069756330625e-07,
      "loss": 1.9472,
      "step": 11
    },
    {
      "epoch": 0.036585365853658534,
      "grad_norm": 1.3359522819519043,
      "learning_rate": 2.866698518872432e-07,
      "loss": 1.9363,
      "step": 12
    },
    {
      "epoch": 0.039634146341463415,
      "grad_norm": 1.0131381750106812,
      "learning_rate": 3.1055900621118013e-07,
      "loss": 1.9448,
      "step": 13
    },
    {
      "epoch": 0.042682926829268296,
      "grad_norm": 1.2646815776824951,
      "learning_rate": 3.3444816053511706e-07,
      "loss": 1.959,
      "step": 14
    },
    {
      "epoch": 0.04573170731707317,
      "grad_norm": 1.3211686611175537,
      "learning_rate": 3.58337314859054e-07,
      "loss": 1.9403,
      "step": 15
    },
    {
      "epoch": 0.04878048780487805,
      "grad_norm": 1.681921124458313,
      "learning_rate": 3.8222646918299094e-07,
      "loss": 1.9031,
      "step": 16
    },
    {
      "epoch": 0.051829268292682924,
      "grad_norm": 1.5285249948501587,
      "learning_rate": 4.0611562350692793e-07,
      "loss": 1.944,
      "step": 17
    },
    {
      "epoch": 0.054878048780487805,
      "grad_norm": 1.5222811698913574,
      "learning_rate": 4.300047778308648e-07,
      "loss": 1.927,
      "step": 18
    },
    {
      "epoch": 0.057926829268292686,
      "grad_norm": 1.0062505006790161,
      "learning_rate": 4.5389393215480175e-07,
      "loss": 1.9614,
      "step": 19
    },
    {
      "epoch": 0.06097560975609756,
      "grad_norm": 1.22685968875885,
      "learning_rate": 4.777830864787387e-07,
      "loss": 1.9503,
      "step": 20
    },
    {
      "epoch": 0.06402439024390244,
      "grad_norm": 1.430120587348938,
      "learning_rate": 5.016722408026756e-07,
      "loss": 1.9664,
      "step": 21
    },
    {
      "epoch": 0.06707317073170732,
      "grad_norm": 1.5091720819473267,
      "learning_rate": 5.255613951266125e-07,
      "loss": 1.9671,
      "step": 22
    },
    {
      "epoch": 0.0701219512195122,
      "grad_norm": 1.91354238986969,
      "learning_rate": 5.494505494505495e-07,
      "loss": 1.9529,
      "step": 23
    },
    {
      "epoch": 0.07317073170731707,
      "grad_norm": 1.3267364501953125,
      "learning_rate": 5.733397037744864e-07,
      "loss": 1.9423,
      "step": 24
    },
    {
      "epoch": 0.07621951219512195,
      "grad_norm": 0.9140681028366089,
      "learning_rate": 5.972288580984234e-07,
      "loss": 1.9326,
      "step": 25
    },
    {
      "epoch": 0.07926829268292683,
      "grad_norm": 1.398987054824829,
      "learning_rate": 6.211180124223603e-07,
      "loss": 1.9437,
      "step": 26
    },
    {
      "epoch": 0.08231707317073171,
      "grad_norm": 1.697360634803772,
      "learning_rate": 6.450071667462972e-07,
      "loss": 1.9477,
      "step": 27
    },
    {
      "epoch": 0.08536585365853659,
      "grad_norm": 1.4342522621154785,
      "learning_rate": 6.688963210702341e-07,
      "loss": 1.9386,
      "step": 28
    },
    {
      "epoch": 0.08841463414634146,
      "grad_norm": 1.227698802947998,
      "learning_rate": 6.92785475394171e-07,
      "loss": 1.9559,
      "step": 29
    },
    {
      "epoch": 0.09146341463414634,
      "grad_norm": 1.42353093624115,
      "learning_rate": 7.16674629718108e-07,
      "loss": 1.9966,
      "step": 30
    },
    {
      "epoch": 0.09451219512195122,
      "grad_norm": 1.276287317276001,
      "learning_rate": 7.405637840420449e-07,
      "loss": 1.9534,
      "step": 31
    },
    {
      "epoch": 0.0975609756097561,
      "grad_norm": 1.4006229639053345,
      "learning_rate": 7.644529383659819e-07,
      "loss": 1.9527,
      "step": 32
    },
    {
      "epoch": 0.10060975609756098,
      "grad_norm": 1.0602002143859863,
      "learning_rate": 7.883420926899189e-07,
      "loss": 1.9661,
      "step": 33
    },
    {
      "epoch": 0.10365853658536585,
      "grad_norm": 0.9262228012084961,
      "learning_rate": 8.122312470138559e-07,
      "loss": 1.9547,
      "step": 34
    },
    {
      "epoch": 0.10670731707317073,
      "grad_norm": 1.1916615962982178,
      "learning_rate": 8.361204013377926e-07,
      "loss": 1.9795,
      "step": 35
    },
    {
      "epoch": 0.10975609756097561,
      "grad_norm": 1.334334373474121,
      "learning_rate": 8.600095556617296e-07,
      "loss": 1.9419,
      "step": 36
    },
    {
      "epoch": 0.11280487804878049,
      "grad_norm": 1.5545203685760498,
      "learning_rate": 8.838987099856666e-07,
      "loss": 1.9478,
      "step": 37
    },
    {
      "epoch": 0.11585365853658537,
      "grad_norm": 1.148835301399231,
      "learning_rate": 9.077878643096035e-07,
      "loss": 1.925,
      "step": 38
    },
    {
      "epoch": 0.11890243902439024,
      "grad_norm": 0.9770706295967102,
      "learning_rate": 9.316770186335405e-07,
      "loss": 1.9417,
      "step": 39
    },
    {
      "epoch": 0.12195121951219512,
      "grad_norm": 1.3688756227493286,
      "learning_rate": 9.555661729574775e-07,
      "loss": 1.9661,
      "step": 40
    },
    {
      "epoch": 0.125,
      "grad_norm": 1.2181131839752197,
      "learning_rate": 9.794553272814141e-07,
      "loss": 1.9664,
      "step": 41
    },
    {
      "epoch": 0.12804878048780488,
      "grad_norm": 1.3802183866500854,
      "learning_rate": 1.0033444816053512e-06,
      "loss": 1.9459,
      "step": 42
    },
    {
      "epoch": 0.13109756097560976,
      "grad_norm": 1.5476137399673462,
      "learning_rate": 1.0272336359292883e-06,
      "loss": 1.9689,
      "step": 43
    },
    {
      "epoch": 0.13414634146341464,
      "grad_norm": 1.2079963684082031,
      "learning_rate": 1.051122790253225e-06,
      "loss": 1.9519,
      "step": 44
    },
    {
      "epoch": 0.13719512195121952,
      "grad_norm": 1.3908147811889648,
      "learning_rate": 1.0750119445771621e-06,
      "loss": 1.9487,
      "step": 45
    },
    {
      "epoch": 0.1402439024390244,
      "grad_norm": 1.446913242340088,
      "learning_rate": 1.098901098901099e-06,
      "loss": 1.9371,
      "step": 46
    },
    {
      "epoch": 0.14329268292682926,
      "grad_norm": 1.2889872789382935,
      "learning_rate": 1.1227902532250359e-06,
      "loss": 1.9686,
      "step": 47
    },
    {
      "epoch": 0.14634146341463414,
      "grad_norm": 0.8432591557502747,
      "learning_rate": 1.1466794075489728e-06,
      "loss": 1.9457,
      "step": 48
    },
    {
      "epoch": 0.14939024390243902,
      "grad_norm": 1.3165030479431152,
      "learning_rate": 1.1705685618729096e-06,
      "loss": 1.9464,
      "step": 49
    },
    {
      "epoch": 0.1524390243902439,
      "grad_norm": 1.5917041301727295,
      "learning_rate": 1.1944577161968467e-06,
      "loss": 1.9593,
      "step": 50
    },
    {
      "epoch": 0.15548780487804878,
      "grad_norm": 1.242082953453064,
      "learning_rate": 1.2183468705207836e-06,
      "loss": 1.9362,
      "step": 51
    },
    {
      "epoch": 0.15853658536585366,
      "grad_norm": 1.5561304092407227,
      "learning_rate": 1.2422360248447205e-06,
      "loss": 1.9341,
      "step": 52
    },
    {
      "epoch": 0.16158536585365854,
      "grad_norm": 1.013962984085083,
      "learning_rate": 1.2661251791686574e-06,
      "loss": 1.9337,
      "step": 53
    },
    {
      "epoch": 0.16463414634146342,
      "grad_norm": 1.2071741819381714,
      "learning_rate": 1.2900143334925945e-06,
      "loss": 1.9285,
      "step": 54
    },
    {
      "epoch": 0.1676829268292683,
      "grad_norm": 1.2972325086593628,
      "learning_rate": 1.3139034878165314e-06,
      "loss": 1.9406,
      "step": 55
    },
    {
      "epoch": 0.17073170731707318,
      "grad_norm": 1.2667003870010376,
      "learning_rate": 1.3377926421404683e-06,
      "loss": 1.9311,
      "step": 56
    },
    {
      "epoch": 0.17378048780487804,
      "grad_norm": 0.9401330351829529,
      "learning_rate": 1.3616817964644054e-06,
      "loss": 1.9355,
      "step": 57
    },
    {
      "epoch": 0.17682926829268292,
      "grad_norm": 1.1516022682189941,
      "learning_rate": 1.385570950788342e-06,
      "loss": 1.9464,
      "step": 58
    },
    {
      "epoch": 0.1798780487804878,
      "grad_norm": 1.2778100967407227,
      "learning_rate": 1.4094601051122791e-06,
      "loss": 1.9242,
      "step": 59
    },
    {
      "epoch": 0.18292682926829268,
      "grad_norm": 1.7733532190322876,
      "learning_rate": 1.433349259436216e-06,
      "loss": 1.9801,
      "step": 60
    },
    {
      "epoch": 0.18597560975609756,
      "grad_norm": 1.1387778520584106,
      "learning_rate": 1.4572384137601529e-06,
      "loss": 1.9177,
      "step": 61
    },
    {
      "epoch": 0.18902439024390244,
      "grad_norm": 1.4622206687927246,
      "learning_rate": 1.4811275680840898e-06,
      "loss": 1.9185,
      "step": 62
    },
    {
      "epoch": 0.19207317073170732,
      "grad_norm": 1.2856990098953247,
      "learning_rate": 1.5050167224080269e-06,
      "loss": 1.9556,
      "step": 63
    },
    {
      "epoch": 0.1951219512195122,
      "grad_norm": 1.1144261360168457,
      "learning_rate": 1.5289058767319638e-06,
      "loss": 1.9334,
      "step": 64
    },
    {
      "epoch": 0.19817073170731708,
      "grad_norm": 1.4428510665893555,
      "learning_rate": 1.5527950310559006e-06,
      "loss": 1.9613,
      "step": 65
    },
    {
      "epoch": 0.20121951219512196,
      "grad_norm": 1.2152093648910522,
      "learning_rate": 1.5766841853798377e-06,
      "loss": 1.9568,
      "step": 66
    },
    {
      "epoch": 0.20426829268292682,
      "grad_norm": 1.603003740310669,
      "learning_rate": 1.6005733397037744e-06,
      "loss": 1.9616,
      "step": 67
    },
    {
      "epoch": 0.2073170731707317,
      "grad_norm": 1.1111464500427246,
      "learning_rate": 1.6244624940277117e-06,
      "loss": 1.949,
      "step": 68
    },
    {
      "epoch": 0.21036585365853658,
      "grad_norm": 1.1285006999969482,
      "learning_rate": 1.6483516483516484e-06,
      "loss": 1.9393,
      "step": 69
    },
    {
      "epoch": 0.21341463414634146,
      "grad_norm": 1.4412013292312622,
      "learning_rate": 1.6722408026755853e-06,
      "loss": 1.9371,
      "step": 70
    },
    {
      "epoch": 0.21646341463414634,
      "grad_norm": 1.5901235342025757,
      "learning_rate": 1.6961299569995224e-06,
      "loss": 1.9191,
      "step": 71
    },
    {
      "epoch": 0.21951219512195122,
      "grad_norm": 1.249669075012207,
      "learning_rate": 1.7200191113234592e-06,
      "loss": 1.9754,
      "step": 72
    },
    {
      "epoch": 0.2225609756097561,
      "grad_norm": 1.0984989404678345,
      "learning_rate": 1.7439082656473961e-06,
      "loss": 1.9097,
      "step": 73
    },
    {
      "epoch": 0.22560975609756098,
      "grad_norm": 1.7193337678909302,
      "learning_rate": 1.7677974199713332e-06,
      "loss": 1.9686,
      "step": 74
    },
    {
      "epoch": 0.22865853658536586,
      "grad_norm": 1.5866273641586304,
      "learning_rate": 1.7916865742952701e-06,
      "loss": 1.927,
      "step": 75
    },
    {
      "epoch": 0.23170731707317074,
      "grad_norm": 1.2322672605514526,
      "learning_rate": 1.815575728619207e-06,
      "loss": 1.919,
      "step": 76
    },
    {
      "epoch": 0.2347560975609756,
      "grad_norm": 1.3303370475769043,
      "learning_rate": 1.839464882943144e-06,
      "loss": 1.9225,
      "step": 77
    },
    {
      "epoch": 0.23780487804878048,
      "grad_norm": 0.9922779202461243,
      "learning_rate": 1.863354037267081e-06,
      "loss": 1.9293,
      "step": 78
    },
    {
      "epoch": 0.24085365853658536,
      "grad_norm": 1.5917788743972778,
      "learning_rate": 1.8872431915910176e-06,
      "loss": 1.9816,
      "step": 79
    },
    {
      "epoch": 0.24390243902439024,
      "grad_norm": 1.5521645545959473,
      "learning_rate": 1.911132345914955e-06,
      "loss": 1.9809,
      "step": 80
    },
    {
      "epoch": 0.24695121951219512,
      "grad_norm": 1.3208634853363037,
      "learning_rate": 1.935021500238892e-06,
      "loss": 1.9643,
      "step": 81
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.9845821857452393,
      "learning_rate": 1.9589106545628283e-06,
      "loss": 1.9421,
      "step": 82
    },
    {
      "epoch": 0.2530487804878049,
      "grad_norm": 1.805677890777588,
      "learning_rate": 1.9827998088867656e-06,
      "loss": 1.9637,
      "step": 83
    },
    {
      "epoch": 0.25609756097560976,
      "grad_norm": 1.329845905303955,
      "learning_rate": 2.0066889632107025e-06,
      "loss": 1.9253,
      "step": 84
    },
    {
      "epoch": 0.25914634146341464,
      "grad_norm": 0.9773482084274292,
      "learning_rate": 2.0305781175346394e-06,
      "loss": 1.9286,
      "step": 85
    },
    {
      "epoch": 0.2621951219512195,
      "grad_norm": 1.344436764717102,
      "learning_rate": 2.0544672718585767e-06,
      "loss": 1.9631,
      "step": 86
    },
    {
      "epoch": 0.2652439024390244,
      "grad_norm": 1.4722480773925781,
      "learning_rate": 2.078356426182513e-06,
      "loss": 1.9576,
      "step": 87
    },
    {
      "epoch": 0.2682926829268293,
      "grad_norm": 1.5130747556686401,
      "learning_rate": 2.10224558050645e-06,
      "loss": 1.9702,
      "step": 88
    },
    {
      "epoch": 0.27134146341463417,
      "grad_norm": 1.4299893379211426,
      "learning_rate": 2.1261347348303873e-06,
      "loss": 1.9693,
      "step": 89
    },
    {
      "epoch": 0.27439024390243905,
      "grad_norm": 1.2989434003829956,
      "learning_rate": 2.1500238891543242e-06,
      "loss": 1.9715,
      "step": 90
    },
    {
      "epoch": 0.2774390243902439,
      "grad_norm": 1.9055125713348389,
      "learning_rate": 2.173913043478261e-06,
      "loss": 1.9095,
      "step": 91
    },
    {
      "epoch": 0.2804878048780488,
      "grad_norm": 1.6924209594726562,
      "learning_rate": 2.197802197802198e-06,
      "loss": 1.9478,
      "step": 92
    },
    {
      "epoch": 0.28353658536585363,
      "grad_norm": 1.173169493675232,
      "learning_rate": 2.221691352126135e-06,
      "loss": 1.9282,
      "step": 93
    },
    {
      "epoch": 0.2865853658536585,
      "grad_norm": 1.710693120956421,
      "learning_rate": 2.2455805064500718e-06,
      "loss": 1.945,
      "step": 94
    },
    {
      "epoch": 0.2896341463414634,
      "grad_norm": 1.3130134344100952,
      "learning_rate": 2.269469660774009e-06,
      "loss": 1.9424,
      "step": 95
    },
    {
      "epoch": 0.2926829268292683,
      "grad_norm": 1.271868348121643,
      "learning_rate": 2.2933588150979455e-06,
      "loss": 1.9504,
      "step": 96
    },
    {
      "epoch": 0.29573170731707316,
      "grad_norm": 1.3181531429290771,
      "learning_rate": 2.3172479694218824e-06,
      "loss": 1.9404,
      "step": 97
    },
    {
      "epoch": 0.29878048780487804,
      "grad_norm": 1.3056014776229858,
      "learning_rate": 2.3411371237458193e-06,
      "loss": 1.9376,
      "step": 98
    },
    {
      "epoch": 0.3018292682926829,
      "grad_norm": 1.6357600688934326,
      "learning_rate": 2.3650262780697566e-06,
      "loss": 1.9781,
      "step": 99
    },
    {
      "epoch": 0.3048780487804878,
      "grad_norm": 1.6848654747009277,
      "learning_rate": 2.3889154323936935e-06,
      "loss": 1.9404,
      "step": 100
    },
    {
      "epoch": 0.3079268292682927,
      "grad_norm": 0.9506048560142517,
      "learning_rate": 2.41280458671763e-06,
      "loss": 1.954,
      "step": 101
    },
    {
      "epoch": 0.31097560975609756,
      "grad_norm": 0.9668936133384705,
      "learning_rate": 2.4366937410415673e-06,
      "loss": 1.9363,
      "step": 102
    },
    {
      "epoch": 0.31402439024390244,
      "grad_norm": 1.4606791734695435,
      "learning_rate": 2.460582895365504e-06,
      "loss": 1.9297,
      "step": 103
    },
    {
      "epoch": 0.3170731707317073,
      "grad_norm": 1.04721200466156,
      "learning_rate": 2.484472049689441e-06,
      "loss": 1.9336,
      "step": 104
    },
    {
      "epoch": 0.3201219512195122,
      "grad_norm": 1.2171670198440552,
      "learning_rate": 2.508361204013378e-06,
      "loss": 1.9476,
      "step": 105
    },
    {
      "epoch": 0.3231707317073171,
      "grad_norm": 1.2751015424728394,
      "learning_rate": 2.5322503583373148e-06,
      "loss": 1.9096,
      "step": 106
    },
    {
      "epoch": 0.32621951219512196,
      "grad_norm": 1.2193056344985962,
      "learning_rate": 2.5561395126612517e-06,
      "loss": 1.959,
      "step": 107
    },
    {
      "epoch": 0.32926829268292684,
      "grad_norm": 1.2362452745437622,
      "learning_rate": 2.580028666985189e-06,
      "loss": 1.9437,
      "step": 108
    },
    {
      "epoch": 0.3323170731707317,
      "grad_norm": 1.4060970544815063,
      "learning_rate": 2.603917821309126e-06,
      "loss": 1.9434,
      "step": 109
    },
    {
      "epoch": 0.3353658536585366,
      "grad_norm": 0.9737282991409302,
      "learning_rate": 2.6278069756330627e-06,
      "loss": 1.9218,
      "step": 110
    },
    {
      "epoch": 0.3384146341463415,
      "grad_norm": 1.5233432054519653,
      "learning_rate": 2.6516961299569996e-06,
      "loss": 1.9773,
      "step": 111
    },
    {
      "epoch": 0.34146341463414637,
      "grad_norm": 1.2265393733978271,
      "learning_rate": 2.6755852842809365e-06,
      "loss": 1.9395,
      "step": 112
    },
    {
      "epoch": 0.3445121951219512,
      "grad_norm": 1.4105302095413208,
      "learning_rate": 2.6994744386048734e-06,
      "loss": 1.9562,
      "step": 113
    },
    {
      "epoch": 0.3475609756097561,
      "grad_norm": 1.0087634325027466,
      "learning_rate": 2.7233635929288107e-06,
      "loss": 1.9419,
      "step": 114
    },
    {
      "epoch": 0.35060975609756095,
      "grad_norm": 0.9560137987136841,
      "learning_rate": 2.747252747252747e-06,
      "loss": 1.9606,
      "step": 115
    },
    {
      "epoch": 0.35365853658536583,
      "grad_norm": 1.3747683763504028,
      "learning_rate": 2.771141901576684e-06,
      "loss": 1.9128,
      "step": 116
    },
    {
      "epoch": 0.3567073170731707,
      "grad_norm": 1.8699897527694702,
      "learning_rate": 2.7950310559006214e-06,
      "loss": 1.9825,
      "step": 117
    },
    {
      "epoch": 0.3597560975609756,
      "grad_norm": 0.8862254023551941,
      "learning_rate": 2.8189202102245582e-06,
      "loss": 1.936,
      "step": 118
    },
    {
      "epoch": 0.3628048780487805,
      "grad_norm": 1.2432398796081543,
      "learning_rate": 2.842809364548495e-06,
      "loss": 1.9601,
      "step": 119
    },
    {
      "epoch": 0.36585365853658536,
      "grad_norm": 1.3888888359069824,
      "learning_rate": 2.866698518872432e-06,
      "loss": 1.9385,
      "step": 120
    },
    {
      "epoch": 0.36890243902439024,
      "grad_norm": 1.117327094078064,
      "learning_rate": 2.890587673196369e-06,
      "loss": 1.9582,
      "step": 121
    },
    {
      "epoch": 0.3719512195121951,
      "grad_norm": 1.319507122039795,
      "learning_rate": 2.9144768275203058e-06,
      "loss": 1.9188,
      "step": 122
    },
    {
      "epoch": 0.375,
      "grad_norm": 1.1676721572875977,
      "learning_rate": 2.938365981844243e-06,
      "loss": 1.9459,
      "step": 123
    },
    {
      "epoch": 0.3780487804878049,
      "grad_norm": 1.0852755308151245,
      "learning_rate": 2.9622551361681795e-06,
      "loss": 1.9162,
      "step": 124
    },
    {
      "epoch": 0.38109756097560976,
      "grad_norm": 1.0964562892913818,
      "learning_rate": 2.9861442904921164e-06,
      "loss": 1.9498,
      "step": 125
    },
    {
      "epoch": 0.38414634146341464,
      "grad_norm": 0.9994953274726868,
      "learning_rate": 3.0100334448160537e-06,
      "loss": 1.9179,
      "step": 126
    },
    {
      "epoch": 0.3871951219512195,
      "grad_norm": 1.2992056608200073,
      "learning_rate": 3.0339225991399906e-06,
      "loss": 1.9501,
      "step": 127
    },
    {
      "epoch": 0.3902439024390244,
      "grad_norm": 1.2953836917877197,
      "learning_rate": 3.0578117534639275e-06,
      "loss": 1.9387,
      "step": 128
    },
    {
      "epoch": 0.3932926829268293,
      "grad_norm": 1.1477677822113037,
      "learning_rate": 3.0817009077878644e-06,
      "loss": 1.9368,
      "step": 129
    },
    {
      "epoch": 0.39634146341463417,
      "grad_norm": 1.1450083255767822,
      "learning_rate": 3.1055900621118013e-06,
      "loss": 1.9316,
      "step": 130
    },
    {
      "epoch": 0.39939024390243905,
      "grad_norm": 1.2700318098068237,
      "learning_rate": 3.1294792164357386e-06,
      "loss": 1.9545,
      "step": 131
    },
    {
      "epoch": 0.4024390243902439,
      "grad_norm": 1.3220826387405396,
      "learning_rate": 3.1533683707596755e-06,
      "loss": 1.9621,
      "step": 132
    },
    {
      "epoch": 0.4054878048780488,
      "grad_norm": 1.0700464248657227,
      "learning_rate": 3.1772575250836123e-06,
      "loss": 1.9432,
      "step": 133
    },
    {
      "epoch": 0.40853658536585363,
      "grad_norm": 1.1687523126602173,
      "learning_rate": 3.201146679407549e-06,
      "loss": 1.9629,
      "step": 134
    },
    {
      "epoch": 0.4115853658536585,
      "grad_norm": 1.203714370727539,
      "learning_rate": 3.2250358337314857e-06,
      "loss": 1.9422,
      "step": 135
    },
    {
      "epoch": 0.4146341463414634,
      "grad_norm": 1.3656145334243774,
      "learning_rate": 3.2489249880554234e-06,
      "loss": 1.9617,
      "step": 136
    },
    {
      "epoch": 0.4176829268292683,
      "grad_norm": 1.152235746383667,
      "learning_rate": 3.2728141423793603e-06,
      "loss": 1.9405,
      "step": 137
    },
    {
      "epoch": 0.42073170731707316,
      "grad_norm": 1.6484832763671875,
      "learning_rate": 3.2967032967032968e-06,
      "loss": 1.9237,
      "step": 138
    },
    {
      "epoch": 0.42378048780487804,
      "grad_norm": 0.869823694229126,
      "learning_rate": 3.3205924510272337e-06,
      "loss": 1.9301,
      "step": 139
    },
    {
      "epoch": 0.4268292682926829,
      "grad_norm": 1.455971598625183,
      "learning_rate": 3.3444816053511705e-06,
      "loss": 1.9295,
      "step": 140
    },
    {
      "epoch": 0.4298780487804878,
      "grad_norm": 1.0484917163848877,
      "learning_rate": 3.3683707596751074e-06,
      "loss": 1.9244,
      "step": 141
    },
    {
      "epoch": 0.4329268292682927,
      "grad_norm": 1.0218935012817383,
      "learning_rate": 3.3922599139990447e-06,
      "loss": 1.9218,
      "step": 142
    },
    {
      "epoch": 0.43597560975609756,
      "grad_norm": 1.1364078521728516,
      "learning_rate": 3.4161490683229816e-06,
      "loss": 1.9319,
      "step": 143
    },
    {
      "epoch": 0.43902439024390244,
      "grad_norm": 1.2479981184005737,
      "learning_rate": 3.4400382226469185e-06,
      "loss": 1.9215,
      "step": 144
    },
    {
      "epoch": 0.4420731707317073,
      "grad_norm": 1.084989070892334,
      "learning_rate": 3.4639273769708554e-06,
      "loss": 1.9458,
      "step": 145
    },
    {
      "epoch": 0.4451219512195122,
      "grad_norm": 1.1187472343444824,
      "learning_rate": 3.4878165312947923e-06,
      "loss": 1.9174,
      "step": 146
    },
    {
      "epoch": 0.4481707317073171,
      "grad_norm": 1.2813881635665894,
      "learning_rate": 3.511705685618729e-06,
      "loss": 1.9304,
      "step": 147
    },
    {
      "epoch": 0.45121951219512196,
      "grad_norm": 1.355372428894043,
      "learning_rate": 3.5355948399426665e-06,
      "loss": 1.9322,
      "step": 148
    },
    {
      "epoch": 0.45426829268292684,
      "grad_norm": 1.1824486255645752,
      "learning_rate": 3.5594839942666033e-06,
      "loss": 1.9262,
      "step": 149
    },
    {
      "epoch": 0.4573170731707317,
      "grad_norm": 1.0536565780639648,
      "learning_rate": 3.5833731485905402e-06,
      "loss": 1.9305,
      "step": 150
    },
    {
      "epoch": 0.4603658536585366,
      "grad_norm": 1.782344937324524,
      "learning_rate": 3.607262302914477e-06,
      "loss": 1.9579,
      "step": 151
    },
    {
      "epoch": 0.4634146341463415,
      "grad_norm": 1.2788569927215576,
      "learning_rate": 3.631151457238414e-06,
      "loss": 1.9147,
      "step": 152
    },
    {
      "epoch": 0.46646341463414637,
      "grad_norm": 1.0829970836639404,
      "learning_rate": 3.6550406115623505e-06,
      "loss": 1.9361,
      "step": 153
    },
    {
      "epoch": 0.4695121951219512,
      "grad_norm": 1.2328369617462158,
      "learning_rate": 3.678929765886288e-06,
      "loss": 1.9373,
      "step": 154
    },
    {
      "epoch": 0.4725609756097561,
      "grad_norm": 1.1621395349502563,
      "learning_rate": 3.702818920210225e-06,
      "loss": 1.9169,
      "step": 155
    },
    {
      "epoch": 0.47560975609756095,
      "grad_norm": 1.0509288311004639,
      "learning_rate": 3.726708074534162e-06,
      "loss": 1.9455,
      "step": 156
    },
    {
      "epoch": 0.47865853658536583,
      "grad_norm": 1.2185430526733398,
      "learning_rate": 3.7505972288580984e-06,
      "loss": 1.9297,
      "step": 157
    },
    {
      "epoch": 0.4817073170731707,
      "grad_norm": 1.222556233406067,
      "learning_rate": 3.7744863831820353e-06,
      "loss": 1.9442,
      "step": 158
    },
    {
      "epoch": 0.4847560975609756,
      "grad_norm": 1.3861175775527954,
      "learning_rate": 3.798375537505972e-06,
      "loss": 1.9516,
      "step": 159
    },
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 1.177702784538269,
      "learning_rate": 3.82226469182991e-06,
      "loss": 1.9354,
      "step": 160
    },
    {
      "epoch": 0.49085365853658536,
      "grad_norm": 1.5396702289581299,
      "learning_rate": 3.846153846153847e-06,
      "loss": 1.9277,
      "step": 161
    },
    {
      "epoch": 0.49390243902439024,
      "grad_norm": 1.6490375995635986,
      "learning_rate": 3.870043000477784e-06,
      "loss": 1.9366,
      "step": 162
    },
    {
      "epoch": 0.4969512195121951,
      "grad_norm": 1.3573803901672363,
      "learning_rate": 3.8939321548017206e-06,
      "loss": 1.9307,
      "step": 163
    },
    {
      "epoch": 0.5,
      "grad_norm": 1.3898566961288452,
      "learning_rate": 3.917821309125657e-06,
      "loss": 1.9151,
      "step": 164
    },
    {
      "epoch": 0.5030487804878049,
      "grad_norm": 1.2588611841201782,
      "learning_rate": 3.9417104634495935e-06,
      "loss": 1.9144,
      "step": 165
    },
    {
      "epoch": 0.5060975609756098,
      "grad_norm": 1.1295562982559204,
      "learning_rate": 3.965599617773531e-06,
      "loss": 1.9298,
      "step": 166
    },
    {
      "epoch": 0.5091463414634146,
      "grad_norm": 1.6052403450012207,
      "learning_rate": 3.989488772097468e-06,
      "loss": 1.9106,
      "step": 167
    },
    {
      "epoch": 0.5121951219512195,
      "grad_norm": 1.455314040184021,
      "learning_rate": 4.013377926421405e-06,
      "loss": 1.9148,
      "step": 168
    },
    {
      "epoch": 0.5152439024390244,
      "grad_norm": 1.221370816230774,
      "learning_rate": 4.037267080745342e-06,
      "loss": 1.9123,
      "step": 169
    },
    {
      "epoch": 0.5182926829268293,
      "grad_norm": 1.3300690650939941,
      "learning_rate": 4.061156235069279e-06,
      "loss": 1.9004,
      "step": 170
    },
    {
      "epoch": 0.5213414634146342,
      "grad_norm": 1.5324848890304565,
      "learning_rate": 4.085045389393216e-06,
      "loss": 1.9269,
      "step": 171
    },
    {
      "epoch": 0.524390243902439,
      "grad_norm": 1.7419891357421875,
      "learning_rate": 4.108934543717153e-06,
      "loss": 1.9058,
      "step": 172
    },
    {
      "epoch": 0.5274390243902439,
      "grad_norm": 1.110393762588501,
      "learning_rate": 4.132823698041089e-06,
      "loss": 1.9101,
      "step": 173
    },
    {
      "epoch": 0.5304878048780488,
      "grad_norm": 1.276220440864563,
      "learning_rate": 4.156712852365026e-06,
      "loss": 1.951,
      "step": 174
    },
    {
      "epoch": 0.5335365853658537,
      "grad_norm": 1.0034399032592773,
      "learning_rate": 4.180602006688963e-06,
      "loss": 1.916,
      "step": 175
    },
    {
      "epoch": 0.5365853658536586,
      "grad_norm": 1.4551115036010742,
      "learning_rate": 4.2044911610129e-06,
      "loss": 1.9527,
      "step": 176
    },
    {
      "epoch": 0.5396341463414634,
      "grad_norm": 1.1432304382324219,
      "learning_rate": 4.228380315336837e-06,
      "loss": 1.9417,
      "step": 177
    },
    {
      "epoch": 0.5426829268292683,
      "grad_norm": 1.3251712322235107,
      "learning_rate": 4.252269469660775e-06,
      "loss": 1.9389,
      "step": 178
    },
    {
      "epoch": 0.5457317073170732,
      "grad_norm": 1.2964903116226196,
      "learning_rate": 4.2761586239847116e-06,
      "loss": 1.9276,
      "step": 179
    },
    {
      "epoch": 0.5487804878048781,
      "grad_norm": 1.4290186166763306,
      "learning_rate": 4.3000477783086484e-06,
      "loss": 1.8939,
      "step": 180
    },
    {
      "epoch": 0.551829268292683,
      "grad_norm": 1.4190891981124878,
      "learning_rate": 4.323936932632585e-06,
      "loss": 1.9231,
      "step": 181
    },
    {
      "epoch": 0.5548780487804879,
      "grad_norm": 0.9606363773345947,
      "learning_rate": 4.347826086956522e-06,
      "loss": 1.9193,
      "step": 182
    },
    {
      "epoch": 0.5579268292682927,
      "grad_norm": 1.2942814826965332,
      "learning_rate": 4.371715241280458e-06,
      "loss": 1.9284,
      "step": 183
    },
    {
      "epoch": 0.5609756097560976,
      "grad_norm": 1.4709460735321045,
      "learning_rate": 4.395604395604396e-06,
      "loss": 1.9509,
      "step": 184
    },
    {
      "epoch": 0.5640243902439024,
      "grad_norm": 1.2656466960906982,
      "learning_rate": 4.419493549928333e-06,
      "loss": 1.8799,
      "step": 185
    },
    {
      "epoch": 0.5670731707317073,
      "grad_norm": 1.399861216545105,
      "learning_rate": 4.44338270425227e-06,
      "loss": 1.9156,
      "step": 186
    },
    {
      "epoch": 0.5701219512195121,
      "grad_norm": 1.3140912055969238,
      "learning_rate": 4.467271858576207e-06,
      "loss": 1.9319,
      "step": 187
    },
    {
      "epoch": 0.573170731707317,
      "grad_norm": 1.2447776794433594,
      "learning_rate": 4.4911610129001435e-06,
      "loss": 1.9137,
      "step": 188
    },
    {
      "epoch": 0.5762195121951219,
      "grad_norm": 1.3533416986465454,
      "learning_rate": 4.51505016722408e-06,
      "loss": 1.9388,
      "step": 189
    },
    {
      "epoch": 0.5792682926829268,
      "grad_norm": 1.1682474613189697,
      "learning_rate": 4.538939321548018e-06,
      "loss": 1.936,
      "step": 190
    },
    {
      "epoch": 0.5823170731707317,
      "grad_norm": 1.0747292041778564,
      "learning_rate": 4.562828475871954e-06,
      "loss": 1.9368,
      "step": 191
    },
    {
      "epoch": 0.5853658536585366,
      "grad_norm": 1.0626906156539917,
      "learning_rate": 4.586717630195891e-06,
      "loss": 1.943,
      "step": 192
    },
    {
      "epoch": 0.5884146341463414,
      "grad_norm": 0.9980499148368835,
      "learning_rate": 4.610606784519828e-06,
      "loss": 1.9351,
      "step": 193
    },
    {
      "epoch": 0.5914634146341463,
      "grad_norm": 1.2597066164016724,
      "learning_rate": 4.634495938843765e-06,
      "loss": 1.9276,
      "step": 194
    },
    {
      "epoch": 0.5945121951219512,
      "grad_norm": 0.90044105052948,
      "learning_rate": 4.658385093167702e-06,
      "loss": 1.9225,
      "step": 195
    },
    {
      "epoch": 0.5975609756097561,
      "grad_norm": 1.7012243270874023,
      "learning_rate": 4.682274247491639e-06,
      "loss": 1.9068,
      "step": 196
    },
    {
      "epoch": 0.600609756097561,
      "grad_norm": 1.2246321439743042,
      "learning_rate": 4.706163401815576e-06,
      "loss": 1.9021,
      "step": 197
    },
    {
      "epoch": 0.6036585365853658,
      "grad_norm": 1.7489140033721924,
      "learning_rate": 4.730052556139513e-06,
      "loss": 1.9087,
      "step": 198
    },
    {
      "epoch": 0.6067073170731707,
      "grad_norm": 1.0586602687835693,
      "learning_rate": 4.75394171046345e-06,
      "loss": 1.9161,
      "step": 199
    },
    {
      "epoch": 0.6097560975609756,
      "grad_norm": 1.2212610244750977,
      "learning_rate": 4.777830864787387e-06,
      "loss": 1.924,
      "step": 200
    },
    {
      "epoch": 0.6128048780487805,
      "grad_norm": 1.3265150785446167,
      "learning_rate": 4.801720019111324e-06,
      "loss": 1.8907,
      "step": 201
    },
    {
      "epoch": 0.6158536585365854,
      "grad_norm": 1.007858157157898,
      "learning_rate": 4.82560917343526e-06,
      "loss": 1.9133,
      "step": 202
    },
    {
      "epoch": 0.6189024390243902,
      "grad_norm": 1.4028000831604004,
      "learning_rate": 4.849498327759198e-06,
      "loss": 1.9252,
      "step": 203
    },
    {
      "epoch": 0.6219512195121951,
      "grad_norm": 1.5757172107696533,
      "learning_rate": 4.8733874820831345e-06,
      "loss": 1.9556,
      "step": 204
    },
    {
      "epoch": 0.625,
      "grad_norm": 1.7050843238830566,
      "learning_rate": 4.897276636407071e-06,
      "loss": 1.9286,
      "step": 205
    },
    {
      "epoch": 0.6280487804878049,
      "grad_norm": 1.480800986289978,
      "learning_rate": 4.921165790731008e-06,
      "loss": 1.9374,
      "step": 206
    },
    {
      "epoch": 0.6310975609756098,
      "grad_norm": 1.3798495531082153,
      "learning_rate": 4.945054945054945e-06,
      "loss": 1.9324,
      "step": 207
    },
    {
      "epoch": 0.6341463414634146,
      "grad_norm": 1.2658337354660034,
      "learning_rate": 4.968944099378882e-06,
      "loss": 1.9412,
      "step": 208
    },
    {
      "epoch": 0.6371951219512195,
      "grad_norm": 1.3130714893341064,
      "learning_rate": 4.99283325370282e-06,
      "loss": 1.9335,
      "step": 209
    },
    {
      "epoch": 0.6402439024390244,
      "grad_norm": 1.5272904634475708,
      "learning_rate": 5.016722408026756e-06,
      "loss": 1.9063,
      "step": 210
    },
    {
      "epoch": 0.6432926829268293,
      "grad_norm": 1.5313810110092163,
      "learning_rate": 5.040611562350693e-06,
      "loss": 1.9213,
      "step": 211
    },
    {
      "epoch": 0.6463414634146342,
      "grad_norm": 1.5184693336486816,
      "learning_rate": 5.0645007166746296e-06,
      "loss": 1.944,
      "step": 212
    },
    {
      "epoch": 0.649390243902439,
      "grad_norm": 1.305742621421814,
      "learning_rate": 5.0883898709985665e-06,
      "loss": 1.9485,
      "step": 213
    },
    {
      "epoch": 0.6524390243902439,
      "grad_norm": 1.8942409753799438,
      "learning_rate": 5.112279025322503e-06,
      "loss": 1.9511,
      "step": 214
    },
    {
      "epoch": 0.6554878048780488,
      "grad_norm": 1.1546876430511475,
      "learning_rate": 5.136168179646441e-06,
      "loss": 1.9329,
      "step": 215
    },
    {
      "epoch": 0.6585365853658537,
      "grad_norm": 1.5213189125061035,
      "learning_rate": 5.160057333970378e-06,
      "loss": 1.9145,
      "step": 216
    },
    {
      "epoch": 0.6615853658536586,
      "grad_norm": 1.2611969709396362,
      "learning_rate": 5.183946488294315e-06,
      "loss": 1.9195,
      "step": 217
    },
    {
      "epoch": 0.6646341463414634,
      "grad_norm": 1.3358227014541626,
      "learning_rate": 5.207835642618252e-06,
      "loss": 1.8946,
      "step": 218
    },
    {
      "epoch": 0.6676829268292683,
      "grad_norm": 1.637919545173645,
      "learning_rate": 5.231724796942189e-06,
      "loss": 1.9252,
      "step": 219
    },
    {
      "epoch": 0.6707317073170732,
      "grad_norm": 1.1602742671966553,
      "learning_rate": 5.2556139512661255e-06,
      "loss": 1.8923,
      "step": 220
    },
    {
      "epoch": 0.6737804878048781,
      "grad_norm": 0.9740343689918518,
      "learning_rate": 5.279503105590062e-06,
      "loss": 1.9194,
      "step": 221
    },
    {
      "epoch": 0.676829268292683,
      "grad_norm": 1.4076448678970337,
      "learning_rate": 5.303392259913999e-06,
      "loss": 1.9264,
      "step": 222
    },
    {
      "epoch": 0.6798780487804879,
      "grad_norm": 1.3798861503601074,
      "learning_rate": 5.327281414237936e-06,
      "loss": 1.9212,
      "step": 223
    },
    {
      "epoch": 0.6829268292682927,
      "grad_norm": 1.2205013036727905,
      "learning_rate": 5.351170568561873e-06,
      "loss": 1.9051,
      "step": 224
    },
    {
      "epoch": 0.6859756097560976,
      "grad_norm": 1.0574887990951538,
      "learning_rate": 5.37505972288581e-06,
      "loss": 1.9168,
      "step": 225
    },
    {
      "epoch": 0.6890243902439024,
      "grad_norm": 1.3339234590530396,
      "learning_rate": 5.398948877209747e-06,
      "loss": 1.8921,
      "step": 226
    },
    {
      "epoch": 0.6920731707317073,
      "grad_norm": 1.111090064048767,
      "learning_rate": 5.4228380315336845e-06,
      "loss": 1.8952,
      "step": 227
    },
    {
      "epoch": 0.6951219512195121,
      "grad_norm": 1.4310805797576904,
      "learning_rate": 5.446727185857621e-06,
      "loss": 1.9149,
      "step": 228
    },
    {
      "epoch": 0.698170731707317,
      "grad_norm": 1.6895301342010498,
      "learning_rate": 5.4706163401815574e-06,
      "loss": 1.948,
      "step": 229
    },
    {
      "epoch": 0.7012195121951219,
      "grad_norm": 2.024019241333008,
      "learning_rate": 5.494505494505494e-06,
      "loss": 1.9095,
      "step": 230
    },
    {
      "epoch": 0.7042682926829268,
      "grad_norm": 1.2090539932250977,
      "learning_rate": 5.518394648829431e-06,
      "loss": 1.9139,
      "step": 231
    },
    {
      "epoch": 0.7073170731707317,
      "grad_norm": 1.0041208267211914,
      "learning_rate": 5.542283803153368e-06,
      "loss": 1.9078,
      "step": 232
    },
    {
      "epoch": 0.7103658536585366,
      "grad_norm": 1.1134798526763916,
      "learning_rate": 5.566172957477306e-06,
      "loss": 1.9233,
      "step": 233
    },
    {
      "epoch": 0.7134146341463414,
      "grad_norm": 1.3305740356445312,
      "learning_rate": 5.590062111801243e-06,
      "loss": 1.9464,
      "step": 234
    },
    {
      "epoch": 0.7164634146341463,
      "grad_norm": 1.243301510810852,
      "learning_rate": 5.61395126612518e-06,
      "loss": 1.9081,
      "step": 235
    },
    {
      "epoch": 0.7195121951219512,
      "grad_norm": 0.9772629141807556,
      "learning_rate": 5.6378404204491165e-06,
      "loss": 1.906,
      "step": 236
    },
    {
      "epoch": 0.7225609756097561,
      "grad_norm": 1.1151772737503052,
      "learning_rate": 5.661729574773053e-06,
      "loss": 1.8856,
      "step": 237
    },
    {
      "epoch": 0.725609756097561,
      "grad_norm": 1.019137978553772,
      "learning_rate": 5.68561872909699e-06,
      "loss": 1.9219,
      "step": 238
    },
    {
      "epoch": 0.7286585365853658,
      "grad_norm": 1.1912275552749634,
      "learning_rate": 5.709507883420927e-06,
      "loss": 1.9038,
      "step": 239
    },
    {
      "epoch": 0.7317073170731707,
      "grad_norm": 1.2323520183563232,
      "learning_rate": 5.733397037744864e-06,
      "loss": 1.9263,
      "step": 240
    },
    {
      "epoch": 0.7347560975609756,
      "grad_norm": 1.2200489044189453,
      "learning_rate": 5.757286192068801e-06,
      "loss": 1.9144,
      "step": 241
    },
    {
      "epoch": 0.7378048780487805,
      "grad_norm": 1.5248363018035889,
      "learning_rate": 5.781175346392738e-06,
      "loss": 1.9369,
      "step": 242
    },
    {
      "epoch": 0.7408536585365854,
      "grad_norm": 1.0916991233825684,
      "learning_rate": 5.805064500716675e-06,
      "loss": 1.8836,
      "step": 243
    },
    {
      "epoch": 0.7439024390243902,
      "grad_norm": 1.0971890687942505,
      "learning_rate": 5.8289536550406116e-06,
      "loss": 1.9299,
      "step": 244
    },
    {
      "epoch": 0.7469512195121951,
      "grad_norm": 1.1186984777450562,
      "learning_rate": 5.852842809364549e-06,
      "loss": 1.9036,
      "step": 245
    },
    {
      "epoch": 0.75,
      "grad_norm": 1.3337066173553467,
      "learning_rate": 5.876731963688486e-06,
      "loss": 1.9098,
      "step": 246
    },
    {
      "epoch": 0.7530487804878049,
      "grad_norm": 1.252734661102295,
      "learning_rate": 5.900621118012423e-06,
      "loss": 1.9281,
      "step": 247
    },
    {
      "epoch": 0.7560975609756098,
      "grad_norm": 1.2734955549240112,
      "learning_rate": 5.924510272336359e-06,
      "loss": 1.9299,
      "step": 248
    },
    {
      "epoch": 0.7591463414634146,
      "grad_norm": 0.9368515014648438,
      "learning_rate": 5.948399426660296e-06,
      "loss": 1.9143,
      "step": 249
    },
    {
      "epoch": 0.7621951219512195,
      "grad_norm": 1.0508118867874146,
      "learning_rate": 5.972288580984233e-06,
      "loss": 1.9146,
      "step": 250
    },
    {
      "epoch": 0.7652439024390244,
      "grad_norm": 1.3169258832931519,
      "learning_rate": 5.996177735308171e-06,
      "loss": 1.8951,
      "step": 251
    },
    {
      "epoch": 0.7682926829268293,
      "grad_norm": 1.4720158576965332,
      "learning_rate": 6.0200668896321075e-06,
      "loss": 1.9096,
      "step": 252
    },
    {
      "epoch": 0.7713414634146342,
      "grad_norm": 0.7764363884925842,
      "learning_rate": 6.043956043956044e-06,
      "loss": 1.9055,
      "step": 253
    },
    {
      "epoch": 0.774390243902439,
      "grad_norm": 1.5512456893920898,
      "learning_rate": 6.067845198279981e-06,
      "loss": 1.9237,
      "step": 254
    },
    {
      "epoch": 0.7774390243902439,
      "grad_norm": 1.321635365486145,
      "learning_rate": 6.091734352603918e-06,
      "loss": 1.8904,
      "step": 255
    },
    {
      "epoch": 0.7804878048780488,
      "grad_norm": 1.0005732774734497,
      "learning_rate": 6.115623506927855e-06,
      "loss": 1.8974,
      "step": 256
    },
    {
      "epoch": 0.7835365853658537,
      "grad_norm": 1.8487420082092285,
      "learning_rate": 6.139512661251792e-06,
      "loss": 1.9189,
      "step": 257
    },
    {
      "epoch": 0.7865853658536586,
      "grad_norm": 1.1215136051177979,
      "learning_rate": 6.163401815575729e-06,
      "loss": 1.8998,
      "step": 258
    },
    {
      "epoch": 0.7896341463414634,
      "grad_norm": 1.438992977142334,
      "learning_rate": 6.187290969899666e-06,
      "loss": 1.9019,
      "step": 259
    },
    {
      "epoch": 0.7926829268292683,
      "grad_norm": 1.183056354522705,
      "learning_rate": 6.2111801242236025e-06,
      "loss": 1.921,
      "step": 260
    },
    {
      "epoch": 0.7957317073170732,
      "grad_norm": 1.1750491857528687,
      "learning_rate": 6.2350692785475394e-06,
      "loss": 1.9189,
      "step": 261
    },
    {
      "epoch": 0.7987804878048781,
      "grad_norm": 1.2113746404647827,
      "learning_rate": 6.258958432871477e-06,
      "loss": 1.9053,
      "step": 262
    },
    {
      "epoch": 0.801829268292683,
      "grad_norm": 1.0733524560928345,
      "learning_rate": 6.282847587195413e-06,
      "loss": 1.9128,
      "step": 263
    },
    {
      "epoch": 0.8048780487804879,
      "grad_norm": 1.0429428815841675,
      "learning_rate": 6.306736741519351e-06,
      "loss": 1.8912,
      "step": 264
    },
    {
      "epoch": 0.8079268292682927,
      "grad_norm": 1.1305841207504272,
      "learning_rate": 6.330625895843287e-06,
      "loss": 1.9158,
      "step": 265
    },
    {
      "epoch": 0.8109756097560976,
      "grad_norm": 1.7175166606903076,
      "learning_rate": 6.354515050167225e-06,
      "loss": 1.9299,
      "step": 266
    },
    {
      "epoch": 0.8140243902439024,
      "grad_norm": 1.5184255838394165,
      "learning_rate": 6.378404204491162e-06,
      "loss": 1.8849,
      "step": 267
    },
    {
      "epoch": 0.8170731707317073,
      "grad_norm": 1.190453052520752,
      "learning_rate": 6.402293358815098e-06,
      "loss": 1.8783,
      "step": 268
    },
    {
      "epoch": 0.8201219512195121,
      "grad_norm": 1.28657865524292,
      "learning_rate": 6.426182513139035e-06,
      "loss": 1.9053,
      "step": 269
    },
    {
      "epoch": 0.823170731707317,
      "grad_norm": 1.048496127128601,
      "learning_rate": 6.450071667462971e-06,
      "loss": 1.9011,
      "step": 270
    },
    {
      "epoch": 0.8262195121951219,
      "grad_norm": 1.3134204149246216,
      "learning_rate": 6.473960821786909e-06,
      "loss": 1.892,
      "step": 271
    },
    {
      "epoch": 0.8292682926829268,
      "grad_norm": 0.9655436873435974,
      "learning_rate": 6.497849976110847e-06,
      "loss": 1.9038,
      "step": 272
    },
    {
      "epoch": 0.8323170731707317,
      "grad_norm": 1.2910897731781006,
      "learning_rate": 6.521739130434783e-06,
      "loss": 1.9027,
      "step": 273
    },
    {
      "epoch": 0.8353658536585366,
      "grad_norm": 1.3932114839553833,
      "learning_rate": 6.545628284758721e-06,
      "loss": 1.8859,
      "step": 274
    },
    {
      "epoch": 0.8384146341463414,
      "grad_norm": 1.0009571313858032,
      "learning_rate": 6.569517439082657e-06,
      "loss": 1.9085,
      "step": 275
    },
    {
      "epoch": 0.8414634146341463,
      "grad_norm": 1.7092267274856567,
      "learning_rate": 6.5934065934065935e-06,
      "loss": 1.9029,
      "step": 276
    },
    {
      "epoch": 0.8445121951219512,
      "grad_norm": 1.1357864141464233,
      "learning_rate": 6.61729574773053e-06,
      "loss": 1.8981,
      "step": 277
    },
    {
      "epoch": 0.8475609756097561,
      "grad_norm": 0.9810603857040405,
      "learning_rate": 6.641184902054467e-06,
      "loss": 1.9068,
      "step": 278
    },
    {
      "epoch": 0.850609756097561,
      "grad_norm": 1.423130750656128,
      "learning_rate": 6.665074056378405e-06,
      "loss": 1.906,
      "step": 279
    },
    {
      "epoch": 0.8536585365853658,
      "grad_norm": 1.0767755508422852,
      "learning_rate": 6.688963210702341e-06,
      "loss": 1.8626,
      "step": 280
    },
    {
      "epoch": 0.8567073170731707,
      "grad_norm": 1.1725006103515625,
      "learning_rate": 6.712852365026279e-06,
      "loss": 1.9086,
      "step": 281
    },
    {
      "epoch": 0.8597560975609756,
      "grad_norm": 1.5360018014907837,
      "learning_rate": 6.736741519350215e-06,
      "loss": 1.9257,
      "step": 282
    },
    {
      "epoch": 0.8628048780487805,
      "grad_norm": 1.2383347749710083,
      "learning_rate": 6.7606306736741526e-06,
      "loss": 1.881,
      "step": 283
    },
    {
      "epoch": 0.8658536585365854,
      "grad_norm": 1.4837791919708252,
      "learning_rate": 6.7845198279980895e-06,
      "loss": 1.8943,
      "step": 284
    },
    {
      "epoch": 0.8689024390243902,
      "grad_norm": 1.0741833448410034,
      "learning_rate": 6.808408982322026e-06,
      "loss": 1.8918,
      "step": 285
    },
    {
      "epoch": 0.8719512195121951,
      "grad_norm": 1.3302024602890015,
      "learning_rate": 6.832298136645963e-06,
      "loss": 1.9013,
      "step": 286
    },
    {
      "epoch": 0.875,
      "grad_norm": 1.2674635648727417,
      "learning_rate": 6.856187290969899e-06,
      "loss": 1.8999,
      "step": 287
    },
    {
      "epoch": 0.8780487804878049,
      "grad_norm": 0.9276521801948547,
      "learning_rate": 6.880076445293837e-06,
      "loss": 1.893,
      "step": 288
    },
    {
      "epoch": 0.8810975609756098,
      "grad_norm": 1.4239391088485718,
      "learning_rate": 6.903965599617773e-06,
      "loss": 1.8887,
      "step": 289
    },
    {
      "epoch": 0.8841463414634146,
      "grad_norm": 1.3026734590530396,
      "learning_rate": 6.927854753941711e-06,
      "loss": 1.8857,
      "step": 290
    },
    {
      "epoch": 0.8871951219512195,
      "grad_norm": 1.2964493036270142,
      "learning_rate": 6.9517439082656485e-06,
      "loss": 1.8859,
      "step": 291
    },
    {
      "epoch": 0.8902439024390244,
      "grad_norm": 1.339819312095642,
      "learning_rate": 6.9756330625895845e-06,
      "loss": 1.8876,
      "step": 292
    },
    {
      "epoch": 0.8932926829268293,
      "grad_norm": 0.8370761871337891,
      "learning_rate": 6.999522216913522e-06,
      "loss": 1.8819,
      "step": 293
    },
    {
      "epoch": 0.8963414634146342,
      "grad_norm": 1.6998356580734253,
      "learning_rate": 7.023411371237458e-06,
      "loss": 1.9235,
      "step": 294
    },
    {
      "epoch": 0.899390243902439,
      "grad_norm": 1.454134225845337,
      "learning_rate": 7.047300525561395e-06,
      "loss": 1.8802,
      "step": 295
    },
    {
      "epoch": 0.9024390243902439,
      "grad_norm": 1.5796455144882202,
      "learning_rate": 7.071189679885333e-06,
      "loss": 1.8845,
      "step": 296
    },
    {
      "epoch": 0.9054878048780488,
      "grad_norm": 1.4182636737823486,
      "learning_rate": 7.095078834209269e-06,
      "loss": 1.9092,
      "step": 297
    },
    {
      "epoch": 0.9085365853658537,
      "grad_norm": 1.569122552871704,
      "learning_rate": 7.118967988533207e-06,
      "loss": 1.8991,
      "step": 298
    },
    {
      "epoch": 0.9115853658536586,
      "grad_norm": 1.0907398462295532,
      "learning_rate": 7.142857142857143e-06,
      "loss": 1.8939,
      "step": 299
    },
    {
      "epoch": 0.9146341463414634,
      "grad_norm": 1.0461949110031128,
      "learning_rate": 7.1667462971810804e-06,
      "loss": 1.8853,
      "step": 300
    },
    {
      "epoch": 0.9176829268292683,
      "grad_norm": 1.1864503622055054,
      "learning_rate": 7.1906354515050165e-06,
      "loss": 1.8943,
      "step": 301
    },
    {
      "epoch": 0.9207317073170732,
      "grad_norm": 1.5050891637802124,
      "learning_rate": 7.214524605828954e-06,
      "loss": 1.8778,
      "step": 302
    },
    {
      "epoch": 0.9237804878048781,
      "grad_norm": 1.042526364326477,
      "learning_rate": 7.238413760152891e-06,
      "loss": 1.8943,
      "step": 303
    },
    {
      "epoch": 0.926829268292683,
      "grad_norm": 1.113753318786621,
      "learning_rate": 7.262302914476828e-06,
      "loss": 1.8764,
      "step": 304
    },
    {
      "epoch": 0.9298780487804879,
      "grad_norm": 1.1224074363708496,
      "learning_rate": 7.286192068800765e-06,
      "loss": 1.882,
      "step": 305
    },
    {
      "epoch": 0.9329268292682927,
      "grad_norm": 1.3235355615615845,
      "learning_rate": 7.310081223124701e-06,
      "loss": 1.9187,
      "step": 306
    },
    {
      "epoch": 0.9359756097560976,
      "grad_norm": 1.1772518157958984,
      "learning_rate": 7.333970377448639e-06,
      "loss": 1.8648,
      "step": 307
    },
    {
      "epoch": 0.9390243902439024,
      "grad_norm": 0.9901207089424133,
      "learning_rate": 7.357859531772576e-06,
      "loss": 1.8921,
      "step": 308
    },
    {
      "epoch": 0.9420731707317073,
      "grad_norm": 1.5677798986434937,
      "learning_rate": 7.381748686096512e-06,
      "loss": 1.8766,
      "step": 309
    },
    {
      "epoch": 0.9451219512195121,
      "grad_norm": 1.306951642036438,
      "learning_rate": 7.40563784042045e-06,
      "loss": 1.906,
      "step": 310
    },
    {
      "epoch": 0.948170731707317,
      "grad_norm": 1.6262617111206055,
      "learning_rate": 7.429526994744386e-06,
      "loss": 1.8828,
      "step": 311
    },
    {
      "epoch": 0.9512195121951219,
      "grad_norm": 1.1887810230255127,
      "learning_rate": 7.453416149068324e-06,
      "loss": 1.8818,
      "step": 312
    },
    {
      "epoch": 0.9542682926829268,
      "grad_norm": 1.2932298183441162,
      "learning_rate": 7.47730530339226e-06,
      "loss": 1.9116,
      "step": 313
    },
    {
      "epoch": 0.9573170731707317,
      "grad_norm": 1.4462857246398926,
      "learning_rate": 7.501194457716197e-06,
      "loss": 1.8905,
      "step": 314
    },
    {
      "epoch": 0.9603658536585366,
      "grad_norm": 0.9686806797981262,
      "learning_rate": 7.5250836120401346e-06,
      "loss": 1.8872,
      "step": 315
    },
    {
      "epoch": 0.9634146341463414,
      "grad_norm": 1.505776286125183,
      "learning_rate": 7.548972766364071e-06,
      "loss": 1.8732,
      "step": 316
    },
    {
      "epoch": 0.9664634146341463,
      "grad_norm": 1.3193867206573486,
      "learning_rate": 7.572861920688008e-06,
      "loss": 1.8779,
      "step": 317
    },
    {
      "epoch": 0.9695121951219512,
      "grad_norm": 1.2116954326629639,
      "learning_rate": 7.596751075011944e-06,
      "loss": 1.8616,
      "step": 318
    },
    {
      "epoch": 0.9725609756097561,
      "grad_norm": 1.2312524318695068,
      "learning_rate": 7.620640229335882e-06,
      "loss": 1.8716,
      "step": 319
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 1.491804838180542,
      "learning_rate": 7.64452938365982e-06,
      "loss": 1.8884,
      "step": 320
    },
    {
      "epoch": 0.9786585365853658,
      "grad_norm": 1.4990068674087524,
      "learning_rate": 7.668418537983756e-06,
      "loss": 1.8668,
      "step": 321
    },
    {
      "epoch": 0.9817073170731707,
      "grad_norm": 1.259024977684021,
      "learning_rate": 7.692307692307694e-06,
      "loss": 1.893,
      "step": 322
    },
    {
      "epoch": 0.9847560975609756,
      "grad_norm": 1.2142138481140137,
      "learning_rate": 7.71619684663163e-06,
      "loss": 1.8928,
      "step": 323
    },
    {
      "epoch": 0.9878048780487805,
      "grad_norm": 1.4802168607711792,
      "learning_rate": 7.740086000955567e-06,
      "loss": 1.8729,
      "step": 324
    },
    {
      "epoch": 0.9908536585365854,
      "grad_norm": 1.6211334466934204,
      "learning_rate": 7.763975155279503e-06,
      "loss": 1.8741,
      "step": 325
    },
    {
      "epoch": 0.9939024390243902,
      "grad_norm": 1.4747838973999023,
      "learning_rate": 7.787864309603441e-06,
      "loss": 1.863,
      "step": 326
    },
    {
      "epoch": 0.9969512195121951,
      "grad_norm": 1.1264011859893799,
      "learning_rate": 7.811753463927377e-06,
      "loss": 1.8874,
      "step": 327
    },
    {
      "epoch": 1.0,
      "grad_norm": 3.634932279586792,
      "learning_rate": 7.835642618251313e-06,
      "loss": 1.8614,
      "step": 328
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.5139472678639664,
      "eval_loss": 1.8722717761993408,
      "eval_model_preparation_time": 0.0015,
      "eval_runtime": 3.2776,
      "eval_samples_per_second": 798.454,
      "eval_steps_per_second": 25.018,
      "step": 328
    }
  ],
  "logging_steps": 1,
  "max_steps": 656,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 58343119806756.0,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
