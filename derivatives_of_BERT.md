# Derivatives of BERT

BERT architecture was the base for various derivative architectures. Few popular BERT derivative architectures are:

- **RoBERTa:**
    - Robistly Optimized BERT Approach
    - It claims that BERT is under-trained
    - RoBERTa training data = 10x BERT training data ***(Training)***
    - Addition of parameters: 15% ***(Architecture)***
    - Removal of NSP task. ***(Training)***
    - Dynamic Masking Pattern which is 4x the masking tasks to learn from.***(Training)***

- **DistilBERT:**

- **ALBERT:**


Each derivative tends to enhance BERT by either altering its architecture and/or its pre-training.